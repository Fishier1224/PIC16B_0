[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Athena (Chou) Mo",
    "section": "",
    "text": "This is Athena’s blog for PIC16B"
  },
  {
    "objectID": "posts/Palmer-Penguins-data-visualization/Untitled.html",
    "href": "posts/Palmer-Penguins-data-visualization/Untitled.html",
    "title": "Palmer Penguins Data Visualization",
    "section": "",
    "text": "Begin by importing all the needed libraries. For databaset modifications, Pandas is required, while it will also be later used for visualization - along with Plotly and Matplotlib.\n\nimport pandas as pd\nfrom plotly import __version__\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nimport matplotlib.pyplot as plt\n\nWith the packages imported, we now read in the dataset (by using the provided url) and name it as penguins:\n\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nNow, specifically for the Species Adelie Penguin (Pygoscelis adeliae), we create a sub-dataframe named adelie_df that isolates data points for delie Penguins from the larger penguins dataframe:\n\nadelie_df = penguins[penguins[\"Species\"] == \"Adelie Penguin (Pygoscelis adeliae)\"].reset_index(drop=True)\nadelie_df.dropna()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n6\nPAL0708\n7\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN4A1\nNo\n11/15/07\n38.9\n17.8\n181.0\n3625.0\nFEMALE\n9.18718\n-25.21799\nNest never observed with full clutch.\n\n\n7\nPAL0708\n8\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN4A2\nNo\n11/15/07\n39.2\n19.6\n195.0\n4675.0\nMALE\n9.46060\n-24.89958\nNest never observed with full clutch.\n\n\n28\nPAL0708\n29\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN18A1\nNo\n11/10/07\n37.9\n18.6\n172.0\n3150.0\nFEMALE\n8.38404\n-25.19837\nNest never observed with full clutch.\n\n\n29\nPAL0708\n30\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN18A2\nNo\n11/10/07\n40.5\n18.9\n180.0\n3950.0\nMALE\n8.90027\n-25.11609\nNest never observed with full clutch.\n\n\n38\nPAL0708\n39\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN25A1\nNo\n11/13/07\n37.6\n19.3\n181.0\n3300.0\nFEMALE\n9.41131\n-25.04169\nNest never observed with full clutch.\n\n\n68\nPAL0809\n69\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN32A1\nNo\n11/11/08\n35.9\n16.6\n190.0\n3050.0\nFEMALE\n8.47781\n-26.07821\nNest never observed with full clutch.\n\n\n69\nPAL0809\n70\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN32A2\nNo\n11/11/08\n41.8\n19.4\n198.0\n4450.0\nMALE\n8.86853\n-26.06209\nNest never observed with full clutch.\n\n\n120\nPAL0910\n121\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN66A1\nNo\n11/17/09\n36.2\n17.2\n187.0\n3150.0\nFEMALE\n9.04296\n-26.19444\nNest never observed with full clutch.\n\n\n121\nPAL0910\n122\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN66A2\nNo\n11/17/09\n37.7\n19.8\n198.0\n3500.0\nMALE\n9.11066\n-26.42563\nNest never observed with full clutch.\n\n\n130\nPAL0910\n131\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN73A1\nNo\n11/23/09\n38.5\n17.9\n190.0\n3325.0\nFEMALE\n8.98460\n-25.57956\nNest never observed with full clutch.\n\n\n131\nPAL0910\n132\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN73A2\nNo\n11/23/09\n43.1\n19.2\n197.0\n3500.0\nMALE\n8.86495\n-26.13960\nNest never observed with full clutch.\n\n\n138\nPAL0910\n139\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN79A1\nNo\n11/16/09\n37.0\n16.5\n185.0\n3400.0\nFEMALE\n8.61651\n-26.07021\nNest never observed with full clutch.\n\n\n139\nPAL0910\n140\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN79A2\nNo\n11/16/09\n39.7\n17.9\n193.0\n4250.0\nMALE\n9.25769\n-25.88798\nNest never observed with full clutch.\n\n\n\n\n\n\n\nNote that the second line in the code block above drops rows for the adelie_df dataframe that includes NaN (incomplete data).\n\n\n\n\n\nFor plotly’s scatter plot, trace is a graph object that includes information about the graph’s x & y values, labels, and marker settings.\nThe following code block defines a new trace named trace1 which is specific to Adelie penguins using the adelie_df that we created before. It assigns “Culmen Length (mm) as the x values and Body Mass (g) as the y values.\nmode=“markers” specifies the drawing mode for the scatter plot. In this case, we are using markers.\nFor the line marker=dict(size=10, symbol=“circle”, line=dict(color=“rgb(0,0,0)”, width=0.5)), it defines the attribute marker (in the trace object) that specifies the design of the markers for the scatter plot. size is set to 10 pixels, and symbol is set to circle. line defines the marker’s outline, which is now black (“rgb(0,0,0)”) and has a width of 0.5.\nThe following line sets the marker color is set to black, and the line below that sets the label name to ““Adelie Penguin”.\n\ntrace1 = go.Scatter(\n    x=adelie_df[\"Culmen Length (mm)\"],\n    y=adelie_df[\"Body Mass (g)\"],\n    mode=\"markers\",\n    marker=dict(size=10, symbol=\"circle\", line=dict(color=\"rgb(0,0,0)\", width=0.5)),\n    marker_color=\"black\",\n    name=\"Adelie Penguin\",\n)\n\nNow we assigns the trace object to the variable data:\n\ndata = trace1\n\nAnd here we define the design for the plot. The title is set to “Culmen Depth (mm) vs. Body Mass (g) for Adelie Penguin” with the legend settings to “True” (which means to include it in the plot). The x-axis title is set to “Culmen Depth (mm)”, and the y-axis title is set to “Body Mass (g)”. The last two lines specifies that the background color for both plot area and the entire plot is transparent (“rgba(0,0,0,0)”).\n\nlayout = dict(\n    title=\"&lt;b&gt;Culmen Depth (mm) vs. Body Mass (g) for Adelie Penguin&lt;/b&gt;\",\n    showlegend=True,\n    xaxis=dict(title=\"Culmen Depth (mm)\"),\n    yaxis=dict(title=\"Body Mass (g)\"),\n    plot_bgcolor=\"rgba(0,0,0,0)\",\n    paper_bgcolor=\"rgba(0,0,0,0)\",\n)\n\nThe first two lines are used to set up the Plotly rendering so that the plots will properly display in JupyterNotebook. Lastly, we create a dictionary fig that includes both data as data and layout as layout. Then the plot is displayed with the line iplot(fig):\n\nimport plotly.io as pio\npio.renderers.default = 'notebook'\n\nfig = dict(data=data, layout=layout)\niplot(fig)\n\n                                                \n\n\n\n\n\n\nSince we are evaluating the Species and Sex variables for all species of penguins, we need to first define a new sub-dataframe that groups the two variables.\nThe following line calls the penguins dataframe and groups the rows from the dataframe based on the columns ‘Species’and ’Sex’. The feature .size() counts the size for each group. The feature .reset_index(name=‘count’) first resets the index of the newly structured dataframe then assigns a new column name count.\n\ngrouped_data = penguins.groupby(['Species', 'Sex']).size().reset_index(name='count')\n\nNow we need to clean the data. Notice how there are three categories for sex: MALE, FEMALE, and “.”\nSince “.” makes invalid information, we are dropping the rows with sex as “.” using the following line:\n\ngrouped_data = grouped_data[grouped_data['Sex'] != \".\"]\n\nSince we want to display the Species on the x-axis and the Sex stacked on the y-axis, we need to pivot the grouped_data using grouped_data.pivot, setting “Species” as the rows, “Sex” as columns, and the values as “count” (which is basically the frequency).\n\npivot_data = grouped_data.pivot(index='Species', columns='Sex', values='count')\n\nNow we plot the bar graph using .plot and setting the plot kind to “bar” and enabling stack with “stacked=True”.\nFor the following three lines, the plot’s x-label is set to “Species”, y-label as “Frequency”, and plot title as “Propotion of Sex by Species”.\nFinally, we run plt.show() to display the plot.\n\npivot_data.plot(kind='bar', stacked=True)\n\nplt.xlabel('Species')\nplt.ylabel('Frequency')\nplt.title('Propotion of Sex by Species')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nMatplotlib has a cool feature for color called colormap. Using the .plot.scatter function, we call the penguins dataframe and sets the x variable as ‘Culmen Length (mm)’ and the y variable as ‘Body Mass (g)’. Notice how another variable (‘Flipper Length (mm)’) is assigned to c. This is used as scale for the colormap. Lastly, the colormap is assigned as ‘viridis’.\nThe following lines, like seen previously, sets the title for the plot and displays it.\n\nax1 = penguins.plot.scatter(x='Culmen Length (mm)', y='Body Mass (g)', c = 'Flipper Length (mm)', colormap='viridis')\n\nplt.title('Culmen Length (mm) vs. Body Mass (g) for all Species')\n\nplt.show()"
  },
  {
    "objectID": "posts/Palmer-Penguins-data-visualization/Untitled.html#preparing-data",
    "href": "posts/Palmer-Penguins-data-visualization/Untitled.html#preparing-data",
    "title": "Palmer Penguins Data Visualization",
    "section": "",
    "text": "Begin by importing all the needed libraries. For databaset modifications, Pandas is required, while it will also be later used for visualization - along with Plotly and Matplotlib.\n\nimport pandas as pd\nfrom plotly import __version__\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nimport matplotlib.pyplot as plt\n\nWith the packages imported, we now read in the dataset (by using the provided url) and name it as penguins:\n\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nNow, specifically for the Species Adelie Penguin (Pygoscelis adeliae), we create a sub-dataframe named adelie_df that isolates data points for delie Penguins from the larger penguins dataframe:\n\nadelie_df = penguins[penguins[\"Species\"] == \"Adelie Penguin (Pygoscelis adeliae)\"].reset_index(drop=True)\nadelie_df.dropna()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n6\nPAL0708\n7\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN4A1\nNo\n11/15/07\n38.9\n17.8\n181.0\n3625.0\nFEMALE\n9.18718\n-25.21799\nNest never observed with full clutch.\n\n\n7\nPAL0708\n8\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN4A2\nNo\n11/15/07\n39.2\n19.6\n195.0\n4675.0\nMALE\n9.46060\n-24.89958\nNest never observed with full clutch.\n\n\n28\nPAL0708\n29\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN18A1\nNo\n11/10/07\n37.9\n18.6\n172.0\n3150.0\nFEMALE\n8.38404\n-25.19837\nNest never observed with full clutch.\n\n\n29\nPAL0708\n30\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN18A2\nNo\n11/10/07\n40.5\n18.9\n180.0\n3950.0\nMALE\n8.90027\n-25.11609\nNest never observed with full clutch.\n\n\n38\nPAL0708\n39\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN25A1\nNo\n11/13/07\n37.6\n19.3\n181.0\n3300.0\nFEMALE\n9.41131\n-25.04169\nNest never observed with full clutch.\n\n\n68\nPAL0809\n69\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN32A1\nNo\n11/11/08\n35.9\n16.6\n190.0\n3050.0\nFEMALE\n8.47781\n-26.07821\nNest never observed with full clutch.\n\n\n69\nPAL0809\n70\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN32A2\nNo\n11/11/08\n41.8\n19.4\n198.0\n4450.0\nMALE\n8.86853\n-26.06209\nNest never observed with full clutch.\n\n\n120\nPAL0910\n121\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN66A1\nNo\n11/17/09\n36.2\n17.2\n187.0\n3150.0\nFEMALE\n9.04296\n-26.19444\nNest never observed with full clutch.\n\n\n121\nPAL0910\n122\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN66A2\nNo\n11/17/09\n37.7\n19.8\n198.0\n3500.0\nMALE\n9.11066\n-26.42563\nNest never observed with full clutch.\n\n\n130\nPAL0910\n131\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN73A1\nNo\n11/23/09\n38.5\n17.9\n190.0\n3325.0\nFEMALE\n8.98460\n-25.57956\nNest never observed with full clutch.\n\n\n131\nPAL0910\n132\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN73A2\nNo\n11/23/09\n43.1\n19.2\n197.0\n3500.0\nMALE\n8.86495\n-26.13960\nNest never observed with full clutch.\n\n\n138\nPAL0910\n139\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN79A1\nNo\n11/16/09\n37.0\n16.5\n185.0\n3400.0\nFEMALE\n8.61651\n-26.07021\nNest never observed with full clutch.\n\n\n139\nPAL0910\n140\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN79A2\nNo\n11/16/09\n39.7\n17.9\n193.0\n4250.0\nMALE\n9.25769\n-25.88798\nNest never observed with full clutch.\n\n\n\n\n\n\n\nNote that the second line in the code block above drops rows for the adelie_df dataframe that includes NaN (incomplete data)."
  },
  {
    "objectID": "posts/Palmer-Penguins-data-visualization/Untitled.html#data-visualization",
    "href": "posts/Palmer-Penguins-data-visualization/Untitled.html#data-visualization",
    "title": "Palmer Penguins Data Visualization",
    "section": "",
    "text": "For plotly’s scatter plot, trace is a graph object that includes information about the graph’s x & y values, labels, and marker settings.\nThe following code block defines a new trace named trace1 which is specific to Adelie penguins using the adelie_df that we created before. It assigns “Culmen Length (mm) as the x values and Body Mass (g) as the y values.\nmode=“markers” specifies the drawing mode for the scatter plot. In this case, we are using markers.\nFor the line marker=dict(size=10, symbol=“circle”, line=dict(color=“rgb(0,0,0)”, width=0.5)), it defines the attribute marker (in the trace object) that specifies the design of the markers for the scatter plot. size is set to 10 pixels, and symbol is set to circle. line defines the marker’s outline, which is now black (“rgb(0,0,0)”) and has a width of 0.5.\nThe following line sets the marker color is set to black, and the line below that sets the label name to ““Adelie Penguin”.\n\ntrace1 = go.Scatter(\n    x=adelie_df[\"Culmen Length (mm)\"],\n    y=adelie_df[\"Body Mass (g)\"],\n    mode=\"markers\",\n    marker=dict(size=10, symbol=\"circle\", line=dict(color=\"rgb(0,0,0)\", width=0.5)),\n    marker_color=\"black\",\n    name=\"Adelie Penguin\",\n)\n\nNow we assigns the trace object to the variable data:\n\ndata = trace1\n\nAnd here we define the design for the plot. The title is set to “Culmen Depth (mm) vs. Body Mass (g) for Adelie Penguin” with the legend settings to “True” (which means to include it in the plot). The x-axis title is set to “Culmen Depth (mm)”, and the y-axis title is set to “Body Mass (g)”. The last two lines specifies that the background color for both plot area and the entire plot is transparent (“rgba(0,0,0,0)”).\n\nlayout = dict(\n    title=\"&lt;b&gt;Culmen Depth (mm) vs. Body Mass (g) for Adelie Penguin&lt;/b&gt;\",\n    showlegend=True,\n    xaxis=dict(title=\"Culmen Depth (mm)\"),\n    yaxis=dict(title=\"Body Mass (g)\"),\n    plot_bgcolor=\"rgba(0,0,0,0)\",\n    paper_bgcolor=\"rgba(0,0,0,0)\",\n)\n\nThe first two lines are used to set up the Plotly rendering so that the plots will properly display in JupyterNotebook. Lastly, we create a dictionary fig that includes both data as data and layout as layout. Then the plot is displayed with the line iplot(fig):\n\nimport plotly.io as pio\npio.renderers.default = 'notebook'\n\nfig = dict(data=data, layout=layout)\niplot(fig)"
  },
  {
    "objectID": "posts/Palmer-Penguins-data-visualization/Untitled.html#with-matplotlib-stacked-bar-plot-propotion-of-sex-by-species",
    "href": "posts/Palmer-Penguins-data-visualization/Untitled.html#with-matplotlib-stacked-bar-plot-propotion-of-sex-by-species",
    "title": "Palmer Penguins Data Visualization",
    "section": "",
    "text": "Since we are evaluating the Species and Sex variables for all species of penguins, we need to first define a new sub-dataframe that groups the two variables.\nThe following line calls the penguins dataframe and groups the rows from the dataframe based on the columns ‘Species’and ’Sex’. The feature .size() counts the size for each group. The feature .reset_index(name=‘count’) first resets the index of the newly structured dataframe then assigns a new column name count.\n\ngrouped_data = penguins.groupby(['Species', 'Sex']).size().reset_index(name='count')\n\nNow we need to clean the data. Notice how there are three categories for sex: MALE, FEMALE, and “.”\nSince “.” makes invalid information, we are dropping the rows with sex as “.” using the following line:\n\ngrouped_data = grouped_data[grouped_data['Sex'] != \".\"]\n\nSince we want to display the Species on the x-axis and the Sex stacked on the y-axis, we need to pivot the grouped_data using grouped_data.pivot, setting “Species” as the rows, “Sex” as columns, and the values as “count” (which is basically the frequency).\n\npivot_data = grouped_data.pivot(index='Species', columns='Sex', values='count')\n\nNow we plot the bar graph using .plot and setting the plot kind to “bar” and enabling stack with “stacked=True”.\nFor the following three lines, the plot’s x-label is set to “Species”, y-label as “Frequency”, and plot title as “Propotion of Sex by Species”.\nFinally, we run plt.show() to display the plot.\n\npivot_data.plot(kind='bar', stacked=True)\n\nplt.xlabel('Species')\nplt.ylabel('Frequency')\nplt.title('Propotion of Sex by Species')\n\nplt.show()"
  },
  {
    "objectID": "posts/Palmer-Penguins-data-visualization/Untitled.html#with-matplotlib-scatter-plot-culmen-length-mm-vs.-body-mass-g-for-all-species",
    "href": "posts/Palmer-Penguins-data-visualization/Untitled.html#with-matplotlib-scatter-plot-culmen-length-mm-vs.-body-mass-g-for-all-species",
    "title": "Palmer Penguins Data Visualization",
    "section": "",
    "text": "Matplotlib has a cool feature for color called colormap. Using the .plot.scatter function, we call the penguins dataframe and sets the x variable as ‘Culmen Length (mm)’ and the y variable as ‘Body Mass (g)’. Notice how another variable (‘Flipper Length (mm)’) is assigned to c. This is used as scale for the colormap. Lastly, the colormap is assigned as ‘viridis’.\nThe following lines, like seen previously, sets the title for the plot and displays it.\n\nax1 = penguins.plot.scatter(x='Culmen Length (mm)', y='Body Mass (g)', c = 'Flipper Length (mm)', colormap='viridis')\n\nplt.title('Culmen Length (mm) vs. Body Mass (g) for all Species')\n\nplt.show()"
  },
  {
    "objectID": "posts/Flask-Messages/HW3.html",
    "href": "posts/Flask-Messages/HW3.html",
    "title": "Flask Messages",
    "section": "",
    "text": "In this blog post, I will be presenting my project on creating a message website with Python Flask. It allows the user to input a message and their name - stored in a SQL database. The website then retrieves, randomly, “n” number of messages from the database to be displayed on the website.\n\n\nTo begin with, we will be focusing on the core script of the project: app.py. First, we import all the necessary libraries. Then we initialize the Flask application with a Flask application instance assigned to “app”.\nfrom flask import Flask, render_template, request, redirect, g\nimport sqlite3\nimport random\n\napp = Flask(__name__)\nNow the following first function get_message_db sets up our database. We use try-except to see if there is an existing g.message_db database, if not, we create one with the line “g.message_db = sqlite3.connect(”messages_db.sqlite”)” and using the cursor. Either way, the function returns the g.message_db database.\ndef get_message_db():\n    \"\"\"\n    retrieves the message database from the global context (g).\n    if does not exist, creates a new database and table.\n    returns g.message_db\n    \"\"\"\n    try:\n        return g.message_db\n    except AttributeError:\n        g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n        cursor = g.message_db.cursor()\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS messages (\n                id INTEGER PRIMARY KEY,\n                handle TEXT,\n                message TEXT\n            )\n        ''')\n        return g.message_db\nThis following function insert_message assigns the user input to “message” and “username” to the variable message and handle. Then the connection is established with the database, and the user inputs are added to the messages table of the database using the cursor. The change is committed, and the connection is then closed.\ndef insert_message(request):\n    \"\"\"\n    inserts a message from the request form to the \"messages\" table\n    of database. The conn is opened beforehand, then closed.\n    returns the handle and message.\n    \"\"\"\n    message = request.form['message']\n    handle = request.form['username']\n    conn = get_message_db()\n    cursor = conn.cursor()\n    cursor.execute('''\n        INSERT INTO messages (handle, message) VALUES (?, ?)\n    ''', (handle, message))\n    conn.commit()\n    conn.close()\n    return handle, message\nThe following random_messages function is a helper function for selecting random messages (rows) from the database which we added to earlier. First, connection is formed with the database and the cursor is used to select from the messages table. We return a randomly selected “n” number of messages. This n value will be assigned a value in the following part of this blog.\ndef random_messages(n):\n    \"\"\"\n    connects to the database and selects a random message from\n    the database. then the connection is closed.\n    inputs \"n\" - number of messages to select\n    returns the randomly selected n number of messages\n    \"\"\"\n    conn = sqlite3.connect(\"messages_db.sqlite\")\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT handle, message FROM messages\")\n    messages = cursor.fetchall()\n    conn.close()\n    return random.sample(messages, min(n, len(messages)))\nThis is our final set-up function before really doing some Flask work. render_view_template calls the random_messages helper function to retrieve the selected function, given the n value of 5. The selected messages are then used to render the “view.html” template - which is used to display the messages on our website.\ndef render_view_template():\n    \"\"\"\n    calls the random_messages function and renders the\n    \"view.html\" template with the selected messages.\n    returns rendered template\n    \"\"\"\n    messages = random_messages(5)  # Grabbing up to 5 random messages\n    return render_template('view.html', messages=messages)\nThe following code beginning with “@app.route” defines the website’s different directories.\nStarting with ‘/view’, we use the view() function to return the function render_view_template that we defined previously to display the random messages.\nThen ‘/submit’ works with the submit.html template, which calls insert message requests for the user to enter their name (under handle) and their message (under message).\nLast but not least, the route ‘/’ just returns the text “home page”. While the last few lines checks for debug before running the app.\n@app.route('/view')\ndef view():\n    return render_view_template()\n\n@app.route('/submit', methods=['GET', 'POST'])\ndef submit():\n    if request.method == 'POST':\n        handle, message = insert_message(request)\n        return render_template('submit.html', submitted=True, handle=handle, message=message)\n    return render_template('submit.html')\n\n@app.route('/')\ndef home():\n    return \"Home Page\"\n\nif __name__ == '__main__':\n    app.run(debug=True)"
  },
  {
    "objectID": "posts/Flask-Messages/HW3.html#app.py",
    "href": "posts/Flask-Messages/HW3.html#app.py",
    "title": "Flask Messages",
    "section": "",
    "text": "To begin with, we will be focusing on the core script of the project: app.py. First, we import all the necessary libraries. Then we initialize the Flask application with a Flask application instance assigned to “app”.\nfrom flask import Flask, render_template, request, redirect, g\nimport sqlite3\nimport random\n\napp = Flask(__name__)\nNow the following first function get_message_db sets up our database. We use try-except to see if there is an existing g.message_db database, if not, we create one with the line “g.message_db = sqlite3.connect(”messages_db.sqlite”)” and using the cursor. Either way, the function returns the g.message_db database.\ndef get_message_db():\n    \"\"\"\n    retrieves the message database from the global context (g).\n    if does not exist, creates a new database and table.\n    returns g.message_db\n    \"\"\"\n    try:\n        return g.message_db\n    except AttributeError:\n        g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n        cursor = g.message_db.cursor()\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS messages (\n                id INTEGER PRIMARY KEY,\n                handle TEXT,\n                message TEXT\n            )\n        ''')\n        return g.message_db\nThis following function insert_message assigns the user input to “message” and “username” to the variable message and handle. Then the connection is established with the database, and the user inputs are added to the messages table of the database using the cursor. The change is committed, and the connection is then closed.\ndef insert_message(request):\n    \"\"\"\n    inserts a message from the request form to the \"messages\" table\n    of database. The conn is opened beforehand, then closed.\n    returns the handle and message.\n    \"\"\"\n    message = request.form['message']\n    handle = request.form['username']\n    conn = get_message_db()\n    cursor = conn.cursor()\n    cursor.execute('''\n        INSERT INTO messages (handle, message) VALUES (?, ?)\n    ''', (handle, message))\n    conn.commit()\n    conn.close()\n    return handle, message\nThe following random_messages function is a helper function for selecting random messages (rows) from the database which we added to earlier. First, connection is formed with the database and the cursor is used to select from the messages table. We return a randomly selected “n” number of messages. This n value will be assigned a value in the following part of this blog.\ndef random_messages(n):\n    \"\"\"\n    connects to the database and selects a random message from\n    the database. then the connection is closed.\n    inputs \"n\" - number of messages to select\n    returns the randomly selected n number of messages\n    \"\"\"\n    conn = sqlite3.connect(\"messages_db.sqlite\")\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT handle, message FROM messages\")\n    messages = cursor.fetchall()\n    conn.close()\n    return random.sample(messages, min(n, len(messages)))\nThis is our final set-up function before really doing some Flask work. render_view_template calls the random_messages helper function to retrieve the selected function, given the n value of 5. The selected messages are then used to render the “view.html” template - which is used to display the messages on our website.\ndef render_view_template():\n    \"\"\"\n    calls the random_messages function and renders the\n    \"view.html\" template with the selected messages.\n    returns rendered template\n    \"\"\"\n    messages = random_messages(5)  # Grabbing up to 5 random messages\n    return render_template('view.html', messages=messages)\nThe following code beginning with “@app.route” defines the website’s different directories.\nStarting with ‘/view’, we use the view() function to return the function render_view_template that we defined previously to display the random messages.\nThen ‘/submit’ works with the submit.html template, which calls insert message requests for the user to enter their name (under handle) and their message (under message).\nLast but not least, the route ‘/’ just returns the text “home page”. While the last few lines checks for debug before running the app.\n@app.route('/view')\ndef view():\n    return render_view_template()\n\n@app.route('/submit', methods=['GET', 'POST'])\ndef submit():\n    if request.method == 'POST':\n        handle, message = insert_message(request)\n        return render_template('submit.html', submitted=True, handle=handle, message=message)\n    return render_template('submit.html')\n\n@app.route('/')\ndef home():\n    return \"Home Page\"\n\nif __name__ == '__main__':\n    app.run(debug=True)"
  },
  {
    "objectID": "posts/Flask-Messages/HW3.html#screenshots-of-app-functions",
    "href": "posts/Flask-Messages/HW3.html#screenshots-of-app-functions",
    "title": "Flask Messages",
    "section": "Screenshots of App Functions",
    "text": "Screenshots of App Functions\n\nfrom IPython.display import Image\nImage(filename='/Users/athena/Desktop/Screenshot1.jpg')\n\n\n\n\n\n\n\n\n\nImage(filename='/Users/athena/Desktop/Screenshot2.jpg')"
  },
  {
    "objectID": "posts/Flask-Messages/HW3.html#link-to-repository",
    "href": "posts/Flask-Messages/HW3.html#link-to-repository",
    "title": "Flask Messages",
    "section": "Link to Repository",
    "text": "Link to Repository\nhttps://github.com/Fishier1224/Flask-Message-Website-"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Web-Scraping/HW2.html",
    "href": "posts/Web-Scraping/HW2.html",
    "title": "Web Scraping on Movie",
    "section": "",
    "text": "In this post, I will be presenting my Scrapy project that is used to scrape my favoriate movie: A Rainy Day in New York - directed by Woody Allen.\nPlease click the following link to see my project repository:\n// Note: The following code is used to display the hyperlink\n\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"\"\"&lt;a href=\"https://github.com/Fishier1224/PIC16B_0/tree/main/TMDB_scraper\"&gt;Project Repository&lt;/a&gt;\"\"\"))\n\n/var/folders/4g/1nylr1s57nvcbk28gd1y9r0h0000gn/T/ipykernel_22301/3278939308.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n  from IPython.core.display import display, HTML\n\n\nProject Repository\n\n\n\n\nBegin by entering the following command in your local terminal “scrapy startproject TMDB_scraper”. My project is named TMDB_scraper, but you can name it whatever you wish. This command will create a folder for your scrapy project.\nInside the project folder, you will see another folder named as TMDB_scraper (along with scrapy.cfg). Click in the folder and open settings.py.\nIn settings.py, you want to include the following code to avoid being blocked by the site because they notice you are scraping (403 error). You may need to first install scrapy-fake-useragent using the following command “pip install scrapy-fake-useragent”.\nThe following code in settings.py turns off the built in UserAgentMiddleware and RetryMiddleware, then enables scrapy-fake-useragent’s RandomUserAgentMiddleware and RetryUserAgentMiddleware.\n\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n    'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,\n    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,\n    'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware': 401,\n}\n\n\nFAKEUSERAGENT_PROVIDERS = [\n    'scrapy_fake_useragent.providers.FakeUserAgentProvider',  # Trying first provider\n    'scrapy_fake_useragent.providers.FakerProvider',  # If FakeUserAgentProvider fails, use faker to generate a user-agent string\n    'scrapy_fake_useragent.providers.FixedUserAgentProvider',  # Fall back to USER_AGENT value\n]\n\n## Set Fallback User-Agent\nUSER_AGENT = 'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'\n\nNotice how there is a fallback User-Agent named USER_AGENT, this is the user agent that the program will fall back on if all providers fails.\n\n\n\nFirst, import scrapy. Then create the class TmdbSpider calling “scrapy.Spider”, and assign the name to “tmdb_spider”.\nNext, under init, specify the start_url with “f”https://www.themoviedb.org/movie/{subdir}/““. Notice how the subdirectory is provided with f string. This means that we will need to use “scrapy crawl tmdb_spider -o movies.csv -a subdir=475303-a-rainy-day-in-new-york” when we are calling scrapy in the terminal later on.\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nNow, our first parse function begins with the cast page of the movie that we are scraping. In this function, we yield the scrapy request to call another function parse_full_credits, which will be used to scape individual casts in the movie. Notice how the url is simply the self.subdir + /cast.\n\n   def parse(self, response):\n        \"\"\"\n        The main parse function.\n        sets the url to the cast page of the movie, then calls parse_full_credits to\n        proceed to individual actors (casts) of the movie.\n        return: N/A\n        \"\"\"\n        yield scrapy.Request(f\"https://www.themoviedb.org/movie/{self.subdir}/cast\",\n                             callback=self.parse_full_credits)\n\nFollowing, we have the parse_full_credits function. Since the cast actor links on the website html are placed within the li element (written like this in the html: ol.people.credits:not(.crew) li), we first select that portion and assign it to actors (note how we also did not(.crew) to avoid also getting the crew cast.\nThen we iterate through the actors to get individual links to the actor’s page using actor.css(‘a::attr(href)’).get(). Next, we check if we are on the actor’s page with if actor_link and calls to the function parse_actor_page for continued parsing.\n\n    def parse_full_credits(self, response):\n        \"\"\"\n        Parsing for actors in the movie, provides link for each.\n        iterates through the &lt;li&gt; elements to get href attributes (link to each actor)\n        calls parse_actor_page() when the actor page is reached.\n        return: N/A\n        \"\"\"\n        actors = response.css('ol.people.credits:not(.crew) li')   # Select the &lt;li&gt; elements containing actor information\n        for actor in actors:\n            actor_link = actor.css('a::attr(href)').get()  # Extract the href attribute of the &lt;a&gt; tag\n            if actor_link:\n                yield scrapy.Request(response.urljoin(actor_link), callback=self.parse_actor_page)\n\nThis is our final parsing function - parse_actor_page. This function assumes that we are at an actor’s page, and begin by extracting the actor’s name. Notice how there is a split in the first line. This is because the website’s html includes an index number for actor’s name and we don’t want that.\nThe second line selects the acting section of the page, where all the actor’s acting works are located. The third line then specifies an xpath, while the four further specifies the xpath in line three to the titles containing “role”. Finally, we iterate through the titles to yield a dictionary where the key is the actor’s name, and the value is the title of individual works that they are known for.\n\n    def parse_actor_page(self, response):\n        \"\"\"\n        Assumes at an actor page. Extracts the actor's name using title::text.\n        For a single actor, extracts the title of their works under the \"known_for\" section\n        iterates through the title to yield a dictionary with actor's name and the name of their work\n        \"\"\"\n        actor_name = response.css('title::text').get().split(' — ')[0] # Split to get rid of following number index\n        acting_section = response.css('h3:contains(\"Acting\")')\n        table = acting_section.xpath('./following-sibling::table').get()\n        titles = scrapy.Selector(text=table).xpath('.//td[contains(@class, \"role\")]/a/bdi/text()').getall()\n        for title in titles:\n            yield {\"actor\": actor_name, 'movie_or_TV_name': title}\n\n\n\n\nNow that we finished implementing the class and three parse functions. Type in the terminal under the same directory as “TMDB_scraper”: scrapy crawl tmdb_spider -o results.csv -a subdir=475303-a-rainy-day-in-new-york.\nThis line will scrape your website and generate a result.csv file that presents the dictionary.\n\n\n\nNow that we have the results.csv, read the csv file using pandas:\n\nimport pandas as pd\nresults = pd.read_csv(\"results.csv\")\n\nHere is a overview of what the result looks like using “results.head()”:\n\nresults.head()\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nTimothée Chalamet\nCall Me by Your Name\n\n\n1\nTimothée Chalamet\nDune\n\n\n2\nTimothée Chalamet\nThe King\n\n\n3\nTimothée Chalamet\nInterstellar\n\n\n4\nTimothée Chalamet\nA Rainy Day in New York\n\n\n\n\n\n\n\nNow we are going to use matplotlib for an interesting visualization. Begin by importing matplotlib:\n\nimport matplotlib.pyplot as plt\n\nNow we create a new dataframe based on the results dataframe, named frequency_df. This line groups the results dataframe by ‘movie_or_TV_name’ and count the number of unique ‘actor’ values (the frequency of actors that shares the same ‘movie_or_TV_name’).\n\nfrequency_df = results.groupby('movie_or_TV_name')['actor'].nunique().reset_index()\n\nHere we rename the columns of our new data frame to “actor” and “frequency”. Then the next line sorts the frequency_df by the ‘frequency’ column in descending order (organizing the movies with the highest frequency to the top of the dataframe).\nThe third line rests the index to make the dataframe more presentable.\n\nfrequency_df = frequency_df.rename(columns={'actor': 'frequency'})\nfrequency_df = frequency_df.sort_values('frequency', ascending=False)\nfrequency_df = frequency_df.reset_index(drop=True)\n\nNext, to prepare for better plotting, we specify the frequency_df to only include the top 10 highest frequency movies.\n\nfrequency_df = frequency_df.head(10)\n\nNow we generate the bar chart! First call the x and y variables as “movie_or_TV_name” and “frequency” in the frequency_df dataframe. Then, label the x and y axis as “Movie or TV Name” and “Frequency”. Finally, we label the plot’s title and set the x-axis ticks as “rotation=90”.\nAt last we display the chart!\n\nplt.bar(frequency_df['movie_or_TV_name'], frequency_df['frequency'])\nplt.xlabel('Movie or TV Name')\nplt.ylabel('Frequency')\nplt.title('Frequency of Actors Sharing the Same Movie or TV Name')\nplt.xticks(rotation=90)\nplt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "posts/Web-Scraping/HW2.html#preparing-for-scraping",
    "href": "posts/Web-Scraping/HW2.html#preparing-for-scraping",
    "title": "Web Scraping on Movie",
    "section": "",
    "text": "Begin by entering the following command in your local terminal “scrapy startproject TMDB_scraper”. My project is named TMDB_scraper, but you can name it whatever you wish. This command will create a folder for your scrapy project.\nInside the project folder, you will see another folder named as TMDB_scraper (along with scrapy.cfg). Click in the folder and open settings.py.\nIn settings.py, you want to include the following code to avoid being blocked by the site because they notice you are scraping (403 error). You may need to first install scrapy-fake-useragent using the following command “pip install scrapy-fake-useragent”.\nThe following code in settings.py turns off the built in UserAgentMiddleware and RetryMiddleware, then enables scrapy-fake-useragent’s RandomUserAgentMiddleware and RetryUserAgentMiddleware.\n\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n    'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,\n    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,\n    'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware': 401,\n}\n\n\nFAKEUSERAGENT_PROVIDERS = [\n    'scrapy_fake_useragent.providers.FakeUserAgentProvider',  # Trying first provider\n    'scrapy_fake_useragent.providers.FakerProvider',  # If FakeUserAgentProvider fails, use faker to generate a user-agent string\n    'scrapy_fake_useragent.providers.FixedUserAgentProvider',  # Fall back to USER_AGENT value\n]\n\n## Set Fallback User-Agent\nUSER_AGENT = 'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'\n\nNotice how there is a fallback User-Agent named USER_AGENT, this is the user agent that the program will fall back on if all providers fails."
  },
  {
    "objectID": "posts/Web-Scraping/HW2.html#implementing-the-tmdbspider-class",
    "href": "posts/Web-Scraping/HW2.html#implementing-the-tmdbspider-class",
    "title": "Web Scraping on Movie",
    "section": "",
    "text": "First, import scrapy. Then create the class TmdbSpider calling “scrapy.Spider”, and assign the name to “tmdb_spider”.\nNext, under init, specify the start_url with “f”https://www.themoviedb.org/movie/{subdir}/““. Notice how the subdirectory is provided with f string. This means that we will need to use “scrapy crawl tmdb_spider -o movies.csv -a subdir=475303-a-rainy-day-in-new-york” when we are calling scrapy in the terminal later on.\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nNow, our first parse function begins with the cast page of the movie that we are scraping. In this function, we yield the scrapy request to call another function parse_full_credits, which will be used to scape individual casts in the movie. Notice how the url is simply the self.subdir + /cast.\n\n   def parse(self, response):\n        \"\"\"\n        The main parse function.\n        sets the url to the cast page of the movie, then calls parse_full_credits to\n        proceed to individual actors (casts) of the movie.\n        return: N/A\n        \"\"\"\n        yield scrapy.Request(f\"https://www.themoviedb.org/movie/{self.subdir}/cast\",\n                             callback=self.parse_full_credits)\n\nFollowing, we have the parse_full_credits function. Since the cast actor links on the website html are placed within the li element (written like this in the html: ol.people.credits:not(.crew) li), we first select that portion and assign it to actors (note how we also did not(.crew) to avoid also getting the crew cast.\nThen we iterate through the actors to get individual links to the actor’s page using actor.css(‘a::attr(href)’).get(). Next, we check if we are on the actor’s page with if actor_link and calls to the function parse_actor_page for continued parsing.\n\n    def parse_full_credits(self, response):\n        \"\"\"\n        Parsing for actors in the movie, provides link for each.\n        iterates through the &lt;li&gt; elements to get href attributes (link to each actor)\n        calls parse_actor_page() when the actor page is reached.\n        return: N/A\n        \"\"\"\n        actors = response.css('ol.people.credits:not(.crew) li')   # Select the &lt;li&gt; elements containing actor information\n        for actor in actors:\n            actor_link = actor.css('a::attr(href)').get()  # Extract the href attribute of the &lt;a&gt; tag\n            if actor_link:\n                yield scrapy.Request(response.urljoin(actor_link), callback=self.parse_actor_page)\n\nThis is our final parsing function - parse_actor_page. This function assumes that we are at an actor’s page, and begin by extracting the actor’s name. Notice how there is a split in the first line. This is because the website’s html includes an index number for actor’s name and we don’t want that.\nThe second line selects the acting section of the page, where all the actor’s acting works are located. The third line then specifies an xpath, while the four further specifies the xpath in line three to the titles containing “role”. Finally, we iterate through the titles to yield a dictionary where the key is the actor’s name, and the value is the title of individual works that they are known for.\n\n    def parse_actor_page(self, response):\n        \"\"\"\n        Assumes at an actor page. Extracts the actor's name using title::text.\n        For a single actor, extracts the title of their works under the \"known_for\" section\n        iterates through the title to yield a dictionary with actor's name and the name of their work\n        \"\"\"\n        actor_name = response.css('title::text').get().split(' — ')[0] # Split to get rid of following number index\n        acting_section = response.css('h3:contains(\"Acting\")')\n        table = acting_section.xpath('./following-sibling::table').get()\n        titles = scrapy.Selector(text=table).xpath('.//td[contains(@class, \"role\")]/a/bdi/text()').getall()\n        for title in titles:\n            yield {\"actor\": actor_name, 'movie_or_TV_name': title}"
  },
  {
    "objectID": "posts/Web-Scraping/HW2.html#running-the-spider",
    "href": "posts/Web-Scraping/HW2.html#running-the-spider",
    "title": "Web Scraping on Movie",
    "section": "",
    "text": "Now that we finished implementing the class and three parse functions. Type in the terminal under the same directory as “TMDB_scraper”: scrapy crawl tmdb_spider -o results.csv -a subdir=475303-a-rainy-day-in-new-york.\nThis line will scrape your website and generate a result.csv file that presents the dictionary."
  },
  {
    "objectID": "posts/Web-Scraping/HW2.html#some-visualization",
    "href": "posts/Web-Scraping/HW2.html#some-visualization",
    "title": "Web Scraping on Movie",
    "section": "",
    "text": "Now that we have the results.csv, read the csv file using pandas:\n\nimport pandas as pd\nresults = pd.read_csv(\"results.csv\")\n\nHere is a overview of what the result looks like using “results.head()”:\n\nresults.head()\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nTimothée Chalamet\nCall Me by Your Name\n\n\n1\nTimothée Chalamet\nDune\n\n\n2\nTimothée Chalamet\nThe King\n\n\n3\nTimothée Chalamet\nInterstellar\n\n\n4\nTimothée Chalamet\nA Rainy Day in New York\n\n\n\n\n\n\n\nNow we are going to use matplotlib for an interesting visualization. Begin by importing matplotlib:\n\nimport matplotlib.pyplot as plt\n\nNow we create a new dataframe based on the results dataframe, named frequency_df. This line groups the results dataframe by ‘movie_or_TV_name’ and count the number of unique ‘actor’ values (the frequency of actors that shares the same ‘movie_or_TV_name’).\n\nfrequency_df = results.groupby('movie_or_TV_name')['actor'].nunique().reset_index()\n\nHere we rename the columns of our new data frame to “actor” and “frequency”. Then the next line sorts the frequency_df by the ‘frequency’ column in descending order (organizing the movies with the highest frequency to the top of the dataframe).\nThe third line rests the index to make the dataframe more presentable.\n\nfrequency_df = frequency_df.rename(columns={'actor': 'frequency'})\nfrequency_df = frequency_df.sort_values('frequency', ascending=False)\nfrequency_df = frequency_df.reset_index(drop=True)\n\nNext, to prepare for better plotting, we specify the frequency_df to only include the top 10 highest frequency movies.\n\nfrequency_df = frequency_df.head(10)\n\nNow we generate the bar chart! First call the x and y variables as “movie_or_TV_name” and “frequency” in the frequency_df dataframe. Then, label the x and y axis as “Movie or TV Name” and “Frequency”. Finally, we label the plot’s title and set the x-axis ticks as “rotation=90”.\nAt last we display the chart!\n\nplt.bar(frequency_df['movie_or_TV_name'], frequency_df['frequency'])\nplt.xlabel('Movie or TV Name')\nplt.ylabel('Frequency')\nplt.title('Frequency of Actors Sharing the Same Movie or TV Name')\nplt.xticks(rotation=90)\nplt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Flask Messages\n\n\n\n\n\n\nHW3\n\n\nflask\n\n\nSQL\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nAthena Mo\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping on Movie\n\n\n\n\n\n\nHW2\n\n\nscrapy\n\n\npandas\n\n\nmatplotlib\n\n\n\n\n\n\n\n\n\nFeb 1, 2024\n\n\nAthena Mo\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins Data Visualization\n\n\n\n\n\n\nHW0\n\n\npandas\n\n\nplotly\n\n\nmatplotlib\n\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\nAthena Mo\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  }
]
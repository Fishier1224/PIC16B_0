[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Athena (Chou) Mo",
    "section": "",
    "text": "This is Athena’s blog for PIC16B"
  },
  {
    "objectID": "posts/Palmer-Penguins-data-visualization/Untitled.html",
    "href": "posts/Palmer-Penguins-data-visualization/Untitled.html",
    "title": "Palmer Penguins Data Visualization",
    "section": "",
    "text": "Begin by importing all the needed libraries. For databaset modifications, Pandas is required, while it will also be later used for visualization - along with Plotly and Matplotlib.\n\nimport pandas as pd\nfrom plotly import __version__\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nimport matplotlib.pyplot as plt\n\nWith the packages imported, we now read in the dataset (by using the provided url) and name it as penguins:\n\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nNow, specifically for the Species Adelie Penguin (Pygoscelis adeliae), we create a sub-dataframe named adelie_df that isolates data points for delie Penguins from the larger penguins dataframe:\n\nadelie_df = penguins[penguins[\"Species\"] == \"Adelie Penguin (Pygoscelis adeliae)\"].reset_index(drop=True)\nadelie_df.dropna()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n6\nPAL0708\n7\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN4A1\nNo\n11/15/07\n38.9\n17.8\n181.0\n3625.0\nFEMALE\n9.18718\n-25.21799\nNest never observed with full clutch.\n\n\n7\nPAL0708\n8\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN4A2\nNo\n11/15/07\n39.2\n19.6\n195.0\n4675.0\nMALE\n9.46060\n-24.89958\nNest never observed with full clutch.\n\n\n28\nPAL0708\n29\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN18A1\nNo\n11/10/07\n37.9\n18.6\n172.0\n3150.0\nFEMALE\n8.38404\n-25.19837\nNest never observed with full clutch.\n\n\n29\nPAL0708\n30\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN18A2\nNo\n11/10/07\n40.5\n18.9\n180.0\n3950.0\nMALE\n8.90027\n-25.11609\nNest never observed with full clutch.\n\n\n38\nPAL0708\n39\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN25A1\nNo\n11/13/07\n37.6\n19.3\n181.0\n3300.0\nFEMALE\n9.41131\n-25.04169\nNest never observed with full clutch.\n\n\n68\nPAL0809\n69\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN32A1\nNo\n11/11/08\n35.9\n16.6\n190.0\n3050.0\nFEMALE\n8.47781\n-26.07821\nNest never observed with full clutch.\n\n\n69\nPAL0809\n70\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN32A2\nNo\n11/11/08\n41.8\n19.4\n198.0\n4450.0\nMALE\n8.86853\n-26.06209\nNest never observed with full clutch.\n\n\n120\nPAL0910\n121\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN66A1\nNo\n11/17/09\n36.2\n17.2\n187.0\n3150.0\nFEMALE\n9.04296\n-26.19444\nNest never observed with full clutch.\n\n\n121\nPAL0910\n122\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN66A2\nNo\n11/17/09\n37.7\n19.8\n198.0\n3500.0\nMALE\n9.11066\n-26.42563\nNest never observed with full clutch.\n\n\n130\nPAL0910\n131\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN73A1\nNo\n11/23/09\n38.5\n17.9\n190.0\n3325.0\nFEMALE\n8.98460\n-25.57956\nNest never observed with full clutch.\n\n\n131\nPAL0910\n132\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN73A2\nNo\n11/23/09\n43.1\n19.2\n197.0\n3500.0\nMALE\n8.86495\n-26.13960\nNest never observed with full clutch.\n\n\n138\nPAL0910\n139\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN79A1\nNo\n11/16/09\n37.0\n16.5\n185.0\n3400.0\nFEMALE\n8.61651\n-26.07021\nNest never observed with full clutch.\n\n\n139\nPAL0910\n140\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN79A2\nNo\n11/16/09\n39.7\n17.9\n193.0\n4250.0\nMALE\n9.25769\n-25.88798\nNest never observed with full clutch.\n\n\n\n\n\n\n\nNote that the second line in the code block above drops rows for the adelie_df dataframe that includes NaN (incomplete data).\n\n\n\n\n\nFor plotly’s scatter plot, trace is a graph object that includes information about the graph’s x & y values, labels, and marker settings.\nThe following code block defines a new trace named trace1 which is specific to Adelie penguins using the adelie_df that we created before. It assigns “Culmen Length (mm) as the x values and Body Mass (g) as the y values.\nmode=“markers” specifies the drawing mode for the scatter plot. In this case, we are using markers.\nFor the line marker=dict(size=10, symbol=“circle”, line=dict(color=“rgb(0,0,0)”, width=0.5)), it defines the attribute marker (in the trace object) that specifies the design of the markers for the scatter plot. size is set to 10 pixels, and symbol is set to circle. line defines the marker’s outline, which is now black (“rgb(0,0,0)”) and has a width of 0.5.\nThe following line sets the marker color is set to black, and the line below that sets the label name to ““Adelie Penguin”.\n\ntrace1 = go.Scatter(\n    x=adelie_df[\"Culmen Length (mm)\"],\n    y=adelie_df[\"Body Mass (g)\"],\n    mode=\"markers\",\n    marker=dict(size=10, symbol=\"circle\", line=dict(color=\"rgb(0,0,0)\", width=0.5)),\n    marker_color=\"black\",\n    name=\"Adelie Penguin\",\n)\n\nNow we assigns the trace object to the variable data:\n\ndata = trace1\n\nAnd here we define the design for the plot. The title is set to “Culmen Depth (mm) vs. Body Mass (g) for Adelie Penguin” with the legend settings to “True” (which means to include it in the plot). The x-axis title is set to “Culmen Depth (mm)”, and the y-axis title is set to “Body Mass (g)”. The last two lines specifies that the background color for both plot area and the entire plot is transparent (“rgba(0,0,0,0)”).\n\nlayout = dict(\n    title=\"&lt;b&gt;Culmen Depth (mm) vs. Body Mass (g) for Adelie Penguin&lt;/b&gt;\",\n    showlegend=True,\n    xaxis=dict(title=\"Culmen Depth (mm)\"),\n    yaxis=dict(title=\"Body Mass (g)\"),\n    plot_bgcolor=\"rgba(0,0,0,0)\",\n    paper_bgcolor=\"rgba(0,0,0,0)\",\n)\n\nThe first two lines are used to set up the Plotly rendering so that the plots will properly display in JupyterNotebook. Lastly, we create a dictionary fig that includes both data as data and layout as layout. Then the plot is displayed with the line iplot(fig):\n\nimport plotly.io as pio\npio.renderers.default = 'notebook'\n\nfig = dict(data=data, layout=layout)\niplot(fig)\n\n                                                \n\n\n\n\n\n\nSince we are evaluating the Species and Sex variables for all species of penguins, we need to first define a new sub-dataframe that groups the two variables.\nThe following line calls the penguins dataframe and groups the rows from the dataframe based on the columns ‘Species’and ’Sex’. The feature .size() counts the size for each group. The feature .reset_index(name=‘count’) first resets the index of the newly structured dataframe then assigns a new column name count.\n\ngrouped_data = penguins.groupby(['Species', 'Sex']).size().reset_index(name='count')\n\nNow we need to clean the data. Notice how there are three categories for sex: MALE, FEMALE, and “.”\nSince “.” makes invalid information, we are dropping the rows with sex as “.” using the following line:\n\ngrouped_data = grouped_data[grouped_data['Sex'] != \".\"]\n\nSince we want to display the Species on the x-axis and the Sex stacked on the y-axis, we need to pivot the grouped_data using grouped_data.pivot, setting “Species” as the rows, “Sex” as columns, and the values as “count” (which is basically the frequency).\n\npivot_data = grouped_data.pivot(index='Species', columns='Sex', values='count')\n\nNow we plot the bar graph using .plot and setting the plot kind to “bar” and enabling stack with “stacked=True”.\nFor the following three lines, the plot’s x-label is set to “Species”, y-label as “Frequency”, and plot title as “Propotion of Sex by Species”.\nFinally, we run plt.show() to display the plot.\n\npivot_data.plot(kind='bar', stacked=True)\n\nplt.xlabel('Species')\nplt.ylabel('Frequency')\nplt.title('Propotion of Sex by Species')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nMatplotlib has a cool feature for color called colormap. Using the .plot.scatter function, we call the penguins dataframe and sets the x variable as ‘Culmen Length (mm)’ and the y variable as ‘Body Mass (g)’. Notice how another variable (‘Flipper Length (mm)’) is assigned to c. This is used as scale for the colormap. Lastly, the colormap is assigned as ‘viridis’.\nThe following lines, like seen previously, sets the title for the plot and displays it.\n\nax1 = penguins.plot.scatter(x='Culmen Length (mm)', y='Body Mass (g)', c = 'Flipper Length (mm)', colormap='viridis')\n\nplt.title('Culmen Length (mm) vs. Body Mass (g) for all Species')\n\nplt.show()"
  },
  {
    "objectID": "posts/Palmer-Penguins-data-visualization/Untitled.html#preparing-data",
    "href": "posts/Palmer-Penguins-data-visualization/Untitled.html#preparing-data",
    "title": "Palmer Penguins Data Visualization",
    "section": "",
    "text": "Begin by importing all the needed libraries. For databaset modifications, Pandas is required, while it will also be later used for visualization - along with Plotly and Matplotlib.\n\nimport pandas as pd\nfrom plotly import __version__\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nimport matplotlib.pyplot as plt\n\nWith the packages imported, we now read in the dataset (by using the provided url) and name it as penguins:\n\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nNow, specifically for the Species Adelie Penguin (Pygoscelis adeliae), we create a sub-dataframe named adelie_df that isolates data points for delie Penguins from the larger penguins dataframe:\n\nadelie_df = penguins[penguins[\"Species\"] == \"Adelie Penguin (Pygoscelis adeliae)\"].reset_index(drop=True)\nadelie_df.dropna()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n6\nPAL0708\n7\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN4A1\nNo\n11/15/07\n38.9\n17.8\n181.0\n3625.0\nFEMALE\n9.18718\n-25.21799\nNest never observed with full clutch.\n\n\n7\nPAL0708\n8\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN4A2\nNo\n11/15/07\n39.2\n19.6\n195.0\n4675.0\nMALE\n9.46060\n-24.89958\nNest never observed with full clutch.\n\n\n28\nPAL0708\n29\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN18A1\nNo\n11/10/07\n37.9\n18.6\n172.0\n3150.0\nFEMALE\n8.38404\n-25.19837\nNest never observed with full clutch.\n\n\n29\nPAL0708\n30\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN18A2\nNo\n11/10/07\n40.5\n18.9\n180.0\n3950.0\nMALE\n8.90027\n-25.11609\nNest never observed with full clutch.\n\n\n38\nPAL0708\n39\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN25A1\nNo\n11/13/07\n37.6\n19.3\n181.0\n3300.0\nFEMALE\n9.41131\n-25.04169\nNest never observed with full clutch.\n\n\n68\nPAL0809\n69\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN32A1\nNo\n11/11/08\n35.9\n16.6\n190.0\n3050.0\nFEMALE\n8.47781\n-26.07821\nNest never observed with full clutch.\n\n\n69\nPAL0809\n70\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN32A2\nNo\n11/11/08\n41.8\n19.4\n198.0\n4450.0\nMALE\n8.86853\n-26.06209\nNest never observed with full clutch.\n\n\n120\nPAL0910\n121\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN66A1\nNo\n11/17/09\n36.2\n17.2\n187.0\n3150.0\nFEMALE\n9.04296\n-26.19444\nNest never observed with full clutch.\n\n\n121\nPAL0910\n122\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN66A2\nNo\n11/17/09\n37.7\n19.8\n198.0\n3500.0\nMALE\n9.11066\n-26.42563\nNest never observed with full clutch.\n\n\n130\nPAL0910\n131\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN73A1\nNo\n11/23/09\n38.5\n17.9\n190.0\n3325.0\nFEMALE\n8.98460\n-25.57956\nNest never observed with full clutch.\n\n\n131\nPAL0910\n132\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN73A2\nNo\n11/23/09\n43.1\n19.2\n197.0\n3500.0\nMALE\n8.86495\n-26.13960\nNest never observed with full clutch.\n\n\n138\nPAL0910\n139\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN79A1\nNo\n11/16/09\n37.0\n16.5\n185.0\n3400.0\nFEMALE\n8.61651\n-26.07021\nNest never observed with full clutch.\n\n\n139\nPAL0910\n140\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN79A2\nNo\n11/16/09\n39.7\n17.9\n193.0\n4250.0\nMALE\n9.25769\n-25.88798\nNest never observed with full clutch.\n\n\n\n\n\n\n\nNote that the second line in the code block above drops rows for the adelie_df dataframe that includes NaN (incomplete data)."
  },
  {
    "objectID": "posts/Palmer-Penguins-data-visualization/Untitled.html#data-visualization",
    "href": "posts/Palmer-Penguins-data-visualization/Untitled.html#data-visualization",
    "title": "Palmer Penguins Data Visualization",
    "section": "",
    "text": "For plotly’s scatter plot, trace is a graph object that includes information about the graph’s x & y values, labels, and marker settings.\nThe following code block defines a new trace named trace1 which is specific to Adelie penguins using the adelie_df that we created before. It assigns “Culmen Length (mm) as the x values and Body Mass (g) as the y values.\nmode=“markers” specifies the drawing mode for the scatter plot. In this case, we are using markers.\nFor the line marker=dict(size=10, symbol=“circle”, line=dict(color=“rgb(0,0,0)”, width=0.5)), it defines the attribute marker (in the trace object) that specifies the design of the markers for the scatter plot. size is set to 10 pixels, and symbol is set to circle. line defines the marker’s outline, which is now black (“rgb(0,0,0)”) and has a width of 0.5.\nThe following line sets the marker color is set to black, and the line below that sets the label name to ““Adelie Penguin”.\n\ntrace1 = go.Scatter(\n    x=adelie_df[\"Culmen Length (mm)\"],\n    y=adelie_df[\"Body Mass (g)\"],\n    mode=\"markers\",\n    marker=dict(size=10, symbol=\"circle\", line=dict(color=\"rgb(0,0,0)\", width=0.5)),\n    marker_color=\"black\",\n    name=\"Adelie Penguin\",\n)\n\nNow we assigns the trace object to the variable data:\n\ndata = trace1\n\nAnd here we define the design for the plot. The title is set to “Culmen Depth (mm) vs. Body Mass (g) for Adelie Penguin” with the legend settings to “True” (which means to include it in the plot). The x-axis title is set to “Culmen Depth (mm)”, and the y-axis title is set to “Body Mass (g)”. The last two lines specifies that the background color for both plot area and the entire plot is transparent (“rgba(0,0,0,0)”).\n\nlayout = dict(\n    title=\"&lt;b&gt;Culmen Depth (mm) vs. Body Mass (g) for Adelie Penguin&lt;/b&gt;\",\n    showlegend=True,\n    xaxis=dict(title=\"Culmen Depth (mm)\"),\n    yaxis=dict(title=\"Body Mass (g)\"),\n    plot_bgcolor=\"rgba(0,0,0,0)\",\n    paper_bgcolor=\"rgba(0,0,0,0)\",\n)\n\nThe first two lines are used to set up the Plotly rendering so that the plots will properly display in JupyterNotebook. Lastly, we create a dictionary fig that includes both data as data and layout as layout. Then the plot is displayed with the line iplot(fig):\n\nimport plotly.io as pio\npio.renderers.default = 'notebook'\n\nfig = dict(data=data, layout=layout)\niplot(fig)"
  },
  {
    "objectID": "posts/Palmer-Penguins-data-visualization/Untitled.html#with-matplotlib-stacked-bar-plot-propotion-of-sex-by-species",
    "href": "posts/Palmer-Penguins-data-visualization/Untitled.html#with-matplotlib-stacked-bar-plot-propotion-of-sex-by-species",
    "title": "Palmer Penguins Data Visualization",
    "section": "",
    "text": "Since we are evaluating the Species and Sex variables for all species of penguins, we need to first define a new sub-dataframe that groups the two variables.\nThe following line calls the penguins dataframe and groups the rows from the dataframe based on the columns ‘Species’and ’Sex’. The feature .size() counts the size for each group. The feature .reset_index(name=‘count’) first resets the index of the newly structured dataframe then assigns a new column name count.\n\ngrouped_data = penguins.groupby(['Species', 'Sex']).size().reset_index(name='count')\n\nNow we need to clean the data. Notice how there are three categories for sex: MALE, FEMALE, and “.”\nSince “.” makes invalid information, we are dropping the rows with sex as “.” using the following line:\n\ngrouped_data = grouped_data[grouped_data['Sex'] != \".\"]\n\nSince we want to display the Species on the x-axis and the Sex stacked on the y-axis, we need to pivot the grouped_data using grouped_data.pivot, setting “Species” as the rows, “Sex” as columns, and the values as “count” (which is basically the frequency).\n\npivot_data = grouped_data.pivot(index='Species', columns='Sex', values='count')\n\nNow we plot the bar graph using .plot and setting the plot kind to “bar” and enabling stack with “stacked=True”.\nFor the following three lines, the plot’s x-label is set to “Species”, y-label as “Frequency”, and plot title as “Propotion of Sex by Species”.\nFinally, we run plt.show() to display the plot.\n\npivot_data.plot(kind='bar', stacked=True)\n\nplt.xlabel('Species')\nplt.ylabel('Frequency')\nplt.title('Propotion of Sex by Species')\n\nplt.show()"
  },
  {
    "objectID": "posts/Palmer-Penguins-data-visualization/Untitled.html#with-matplotlib-scatter-plot-culmen-length-mm-vs.-body-mass-g-for-all-species",
    "href": "posts/Palmer-Penguins-data-visualization/Untitled.html#with-matplotlib-scatter-plot-culmen-length-mm-vs.-body-mass-g-for-all-species",
    "title": "Palmer Penguins Data Visualization",
    "section": "",
    "text": "Matplotlib has a cool feature for color called colormap. Using the .plot.scatter function, we call the penguins dataframe and sets the x variable as ‘Culmen Length (mm)’ and the y variable as ‘Body Mass (g)’. Notice how another variable (‘Flipper Length (mm)’) is assigned to c. This is used as scale for the colormap. Lastly, the colormap is assigned as ‘viridis’.\nThe following lines, like seen previously, sets the title for the plot and displays it.\n\nax1 = penguins.plot.scatter(x='Culmen Length (mm)', y='Body Mass (g)', c = 'Flipper Length (mm)', colormap='viridis')\n\nplt.title('Culmen Length (mm) vs. Body Mass (g) for all Species')\n\nplt.show()"
  },
  {
    "objectID": "posts/Flask-Messages/HW3.html",
    "href": "posts/Flask-Messages/HW3.html",
    "title": "Flask Messages",
    "section": "",
    "text": "In this blog post, I will be presenting my project on creating a message website with Python Flask. It allows the user to input a message and their name - stored in a SQL database. The website then retrieves, randomly, “n” number of messages from the database to be displayed on the website.\n\n\nTo begin with, we will be focusing on the core script of the project: app.py. First, we import all the necessary libraries. Then we initialize the Flask application with a Flask application instance assigned to “app”.\nfrom flask import Flask, render_template, request, redirect, g\nimport sqlite3\nimport random\n\napp = Flask(__name__)\nNow the following first function get_message_db sets up our database. We use try-except to see if there is an existing g.message_db database, if not, we create one with the line “g.message_db = sqlite3.connect(”messages_db.sqlite”)” and using the cursor. Either way, the function returns the g.message_db database.\ndef get_message_db():\n    \"\"\"\n    retrieves the message database from the global context (g).\n    if does not exist, creates a new database and table.\n    returns g.message_db\n    \"\"\"\n    try:\n        return g.message_db\n    except AttributeError:\n        g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n        cursor = g.message_db.cursor()\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS messages (\n                id INTEGER PRIMARY KEY,\n                handle TEXT,\n                message TEXT\n            )\n        ''')\n        return g.message_db\nThis following function insert_message assigns the user input to “message” and “username” to the variable message and handle. Then the connection is established with the database, and the user inputs are added to the messages table of the database using the cursor. The change is committed, and the connection is then closed.\ndef insert_message(request):\n    \"\"\"\n    inserts a message from the request form to the \"messages\" table\n    of database. The conn is opened beforehand, then closed.\n    returns the handle and message.\n    \"\"\"\n    message = request.form['message']\n    handle = request.form['username']\n    conn = get_message_db()\n    cursor = conn.cursor()\n    cursor.execute('''\n        INSERT INTO messages (handle, message) VALUES (?, ?)\n    ''', (handle, message))\n    conn.commit()\n    conn.close()\n    return handle, message\nThe following random_messages function is a helper function for selecting random messages (rows) from the database which we added to earlier. First, connection is formed with the database and the cursor is used to select from the messages table. We return a randomly selected “n” number of messages. This n value will be assigned a value in the following part of this blog.\ndef random_messages(n):\n    \"\"\"\n    connects to the database and selects a random message from\n    the database. then the connection is closed.\n    inputs \"n\" - number of messages to select\n    returns the randomly selected n number of messages\n    \"\"\"\n    conn = sqlite3.connect(\"messages_db.sqlite\")\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT handle, message FROM messages\")\n    messages = cursor.fetchall()\n    conn.close()\n    return random.sample(messages, min(n, len(messages)))\nThis is our final set-up function before really doing some Flask work. render_view_template calls the random_messages helper function to retrieve the selected function, given the n value of 5. The selected messages are then used to render the “view.html” template - which is used to display the messages on our website.\ndef render_view_template():\n    \"\"\"\n    calls the random_messages function and renders the\n    \"view.html\" template with the selected messages.\n    returns rendered template\n    \"\"\"\n    messages = random_messages(5)  # Grabbing up to 5 random messages\n    return render_template('view.html', messages=messages)\nThe following code beginning with “@app.route” defines the website’s different directories.\nStarting with ‘/view’, we use the view() function to return the function render_view_template that we defined previously to display the random messages.\nThen ‘/submit’ works with the submit.html template, which calls insert message requests for the user to enter their name (under handle) and their message (under message).\nLast but not least, the route ‘/’ just returns the text “home page”. While the last few lines checks for debug before running the app.\n@app.route('/view')\ndef view():\n    return render_view_template()\n\n@app.route('/submit', methods=['GET', 'POST'])\ndef submit():\n    if request.method == 'POST':\n        handle, message = insert_message(request)\n        return render_template('submit.html', submitted=True, handle=handle, message=message)\n    return render_template('submit.html')\n\n@app.route('/')\ndef home():\n    return \"Home Page\"\n\nif __name__ == '__main__':\n    app.run(debug=True)"
  },
  {
    "objectID": "posts/Flask-Messages/HW3.html#app.py",
    "href": "posts/Flask-Messages/HW3.html#app.py",
    "title": "Flask Messages",
    "section": "",
    "text": "To begin with, we will be focusing on the core script of the project: app.py. First, we import all the necessary libraries. Then we initialize the Flask application with a Flask application instance assigned to “app”.\nfrom flask import Flask, render_template, request, redirect, g\nimport sqlite3\nimport random\n\napp = Flask(__name__)\nNow the following first function get_message_db sets up our database. We use try-except to see if there is an existing g.message_db database, if not, we create one with the line “g.message_db = sqlite3.connect(”messages_db.sqlite”)” and using the cursor. Either way, the function returns the g.message_db database.\ndef get_message_db():\n    \"\"\"\n    retrieves the message database from the global context (g).\n    if does not exist, creates a new database and table.\n    returns g.message_db\n    \"\"\"\n    try:\n        return g.message_db\n    except AttributeError:\n        g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n        cursor = g.message_db.cursor()\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS messages (\n                id INTEGER PRIMARY KEY,\n                handle TEXT,\n                message TEXT\n            )\n        ''')\n        return g.message_db\nThis following function insert_message assigns the user input to “message” and “username” to the variable message and handle. Then the connection is established with the database, and the user inputs are added to the messages table of the database using the cursor. The change is committed, and the connection is then closed.\ndef insert_message(request):\n    \"\"\"\n    inserts a message from the request form to the \"messages\" table\n    of database. The conn is opened beforehand, then closed.\n    returns the handle and message.\n    \"\"\"\n    message = request.form['message']\n    handle = request.form['username']\n    conn = get_message_db()\n    cursor = conn.cursor()\n    cursor.execute('''\n        INSERT INTO messages (handle, message) VALUES (?, ?)\n    ''', (handle, message))\n    conn.commit()\n    conn.close()\n    return handle, message\nThe following random_messages function is a helper function for selecting random messages (rows) from the database which we added to earlier. First, connection is formed with the database and the cursor is used to select from the messages table. We return a randomly selected “n” number of messages. This n value will be assigned a value in the following part of this blog.\ndef random_messages(n):\n    \"\"\"\n    connects to the database and selects a random message from\n    the database. then the connection is closed.\n    inputs \"n\" - number of messages to select\n    returns the randomly selected n number of messages\n    \"\"\"\n    conn = sqlite3.connect(\"messages_db.sqlite\")\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT handle, message FROM messages\")\n    messages = cursor.fetchall()\n    conn.close()\n    return random.sample(messages, min(n, len(messages)))\nThis is our final set-up function before really doing some Flask work. render_view_template calls the random_messages helper function to retrieve the selected function, given the n value of 5. The selected messages are then used to render the “view.html” template - which is used to display the messages on our website.\ndef render_view_template():\n    \"\"\"\n    calls the random_messages function and renders the\n    \"view.html\" template with the selected messages.\n    returns rendered template\n    \"\"\"\n    messages = random_messages(5)  # Grabbing up to 5 random messages\n    return render_template('view.html', messages=messages)\nThe following code beginning with “@app.route” defines the website’s different directories.\nStarting with ‘/view’, we use the view() function to return the function render_view_template that we defined previously to display the random messages.\nThen ‘/submit’ works with the submit.html template, which calls insert message requests for the user to enter their name (under handle) and their message (under message).\nLast but not least, the route ‘/’ just returns the text “home page”. While the last few lines checks for debug before running the app.\n@app.route('/view')\ndef view():\n    return render_view_template()\n\n@app.route('/submit', methods=['GET', 'POST'])\ndef submit():\n    if request.method == 'POST':\n        handle, message = insert_message(request)\n        return render_template('submit.html', submitted=True, handle=handle, message=message)\n    return render_template('submit.html')\n\n@app.route('/')\ndef home():\n    return \"Home Page\"\n\nif __name__ == '__main__':\n    app.run(debug=True)"
  },
  {
    "objectID": "posts/Flask-Messages/HW3.html#screenshots-of-app-functions",
    "href": "posts/Flask-Messages/HW3.html#screenshots-of-app-functions",
    "title": "Flask Messages",
    "section": "Screenshots of App Functions",
    "text": "Screenshots of App Functions\n\nfrom IPython.display import Image\nImage(filename='/Users/athena/Desktop/Screenshot1.jpg')\n\n\n\n\n\n\n\n\n\nImage(filename='/Users/athena/Desktop/Screenshot2.jpg')"
  },
  {
    "objectID": "posts/Flask-Messages/HW3.html#link-to-repository",
    "href": "posts/Flask-Messages/HW3.html#link-to-repository",
    "title": "Flask Messages",
    "section": "Link to Repository",
    "text": "Link to Repository\nhttps://github.com/Fishier1224/Flask-Message-Website-"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/fake-news/hw6.html",
    "href": "posts/fake-news/hw6.html",
    "title": "Fake News Classification with Keras",
    "section": "",
    "text": "In this blog, I will be sharing with you my three models (focusing on different aspects) on fake news classification. The dataset that is used consists of labelled points. Each row of the data includes: title of the article, full article text, and its label (0 if the article is true and 1 if the article contains fake news).\n\n\nFirst, we begin by importing necessary libraries such as Pandas for data manipulation, TensorFlow for building the neural network model, NLTK for natural language processing (stopwords), and scikit-learn for splitting the dataset. Next, we load the dataset from a CSV file hosted on GitHub and download the NLTK stopwords - this will be used later.\nThe function calculate_vocabulary_size is used to compute the size of the vocabulary in the text data, which is the number of unique words. This function is applied to both the title and text columns of the dataset.\nThe function make_dataset preprocesses the text data by converting it to lowercase, removing stopwords, tokenizing, and padding sequences to a fixed length. Following the function, we create a TensorFlow Dataset object from the preprocessed data and splits it into training and validation sets using the train_test_split function from scikit-learn.\nFinally, we calculate the base rate for the dataset, which is the proportion of the majority class (fake or real news) in the training set.\nThe following code block prints the vocabulary size and base rate.\nimport pandas as pd\nimport tensorflow as tf\nfrom keras.layers import Input, Embedding, Dense, Concatenate, Dropout, SimpleRNN, GRU\nfrom keras.models import Model\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Load the dataset\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\ndf = pd.read_csv(train_url)\n\n# Download NLTK stopwords\nimport nltk\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\n# Calculate the size of the vocabulary\ndef calculate_vocabulary_size(text_column):\n    all_words = ' '.join(text_column).split()\n    unique_words = set(all_words)\n    return len(unique_words)\n\n# Calculate the size of the vocabulary for both title and text columns\nsize_vocabulary_title = calculate_vocabulary_size(df['title'])\nsize_vocabulary_text = calculate_vocabulary_size(df['text'])\n\n# Use the maximum vocabulary size from title and text columns\nsize_vocabulary = max(size_vocabulary_title, size_vocabulary_text)\n\nprint(\"Vocabulary Size:\", size_vocabulary)\n\ndef make_dataset(df):\n    # Lowercase the text and title columns\n    df['text'] = df['text'].str.lower()\n    df['title'] = df['title'].str.lower()\n\n    # Remove stopwords from the text and title columns using NLTK\n    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n    df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n\n    # Tokenize the text and title columns\n    tokenizer = Tokenizer(num_words=size_vocabulary)\n    tokenizer.fit_on_texts(df['text'].values)\n    tokenizer.fit_on_texts(df['title'].values)\n\n    # Convert text to sequences and pad them\n    text_sequences = pad_sequences(tokenizer.texts_to_sequences(df['text'].values))\n    title_sequences = pad_sequences(tokenizer.texts_to_sequences(df['title'].values))\n\n    # Combine title and text sequences into a tuple\n    sequences = (title_sequences, text_sequences)\n\n    # Convert the DataFrame to a TensorFlow Dataset\n    dataset = tf.data.Dataset.from_tensor_slices((sequences, df['fake'].values))\n\n    return dataset\n\n# Create datasets\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\ntrain_dataset = make_dataset(train_df).batch(100)\nval_dataset = make_dataset(val_df).batch(100)\n\n# Calculate the base rate for the dataset\nbase_rate = max(train_df['fake'].value_counts(normalize=True))\n\nprint(\"Base Rate:\", base_rate)\nHere is the output:\nVocabulary Size: 276283\nBase Rate: 0.5222451138704828\n\n\n\nNow we define three separate models for classifying fake news based on the article title, text, or both.\nFirstly, we define the shared layers - an Embedding layer and an RNN layer.\nThen, three models are defined and, for each model, the code follows a similar structure:\n\nfirst define shared layers, which consist of an embedding layer and a recurrent neural network (RNN) layer. The embedding layer converts each word index into a dense vector representation, and the RNN layer processes sequences of these word embeddings.\nThen we define separate inputs for the title and the text, followed by their respective embedding and RNN layers.\nFor the model considering both title and text, we concatenate the outputs of the RNN layers for the title and text inputs.\nFinally, we add a dense layer with a sigmoid activation function, which is typical for binary classification tasks, to produce the final output of each model.\nAdditionally, dropout layers with a dropout rate of 0.5 are inserted after the RNN layers for the text inputs, which helps prevent overfitting by randomly dropping out (setting to zero) a fraction of input units during training.\n\n# Define the shared layers\nembedding_layer = Embedding(input_dim=size_vocabulary, output_dim=100)\nrnn_layer = SimpleRNN(128, return_sequences=True)  # Change LSTM to SimpleRNN or GRU\n\n# Model using only the article title\ntitle_input = Input(shape=(None,))\ntitle_embedding = embedding_layer(title_input)\ntitle_rnn = rnn_layer(title_embedding)\ntitle_rnn = SimpleRNN(128)(title_rnn)  # Change LSTM to SimpleRNN or GRU\ntitle_output = Dense(1, activation='sigmoid')(title_rnn)\n\ntitle_model = Model(inputs=title_input, outputs=title_output)\n\n\n# Model using only the article text\ntext_input = Input(shape=(None,))\ntext_embedding = embedding_layer(text_input)\ntext_rnn = rnn_layer(text_embedding)\ntext_dropout = Dropout(0.5)(text_rnn)  # Add dropout layer with dropout rate of 0.5\ntext_rnn = SimpleRNN(128)(text_dropout)  # Change LSTM to SimpleRNN or GRU\ntext_output = Dense(1, activation='sigmoid')(text_rnn)\n\ntext_model = Model(inputs=text_input, outputs=text_output)\n\n# Model using both article title and text\ntitle_input = Input(shape=(None,))\ntext_input = Input(shape=(None,))\n\ntitle_embedding = embedding_layer(title_input)\ntext_embedding = embedding_layer(text_input)\n\ntitle_rnn = rnn_layer(title_embedding)\ntitle_dropout = Dropout(0.5)(title_rnn)  # Add dropout layer with dropout rate of 0.5\ntitle_rnn = SimpleRNN(128)(title_dropout)  # Change LSTM to SimpleRNN or GRU\n\ntext_rnn = rnn_layer(text_embedding)\ntext_dropout = Dropout(0.5)(text_rnn)  # Add dropout layer with dropout rate of 0.5\ntext_rnn = SimpleRNN(128)(text_dropout)  # Change LSTM to SimpleRNN or GRU\n\nconcatenated = Concatenate()([title_rnn, text_rnn])\ncombined_output = Dense(1, activation='sigmoid')(concatenated)\n\ncombined_model = Model(inputs=[title_input, text_input], outputs=combined_output)\n\n\n\nNow we compile and train the three models for text classification tasks.\nFirst, the compile() function configures each model for training by specifying the optimizer, loss function, and evaluation metrics. In this case, the Adam optimizer is chosen, along with binary cross-entropy loss, which is typical for binary classification problems, and accuracy as the metric to monitor during training.\nFollowing compilation, the fit() function is called for each model to train them on the provided data. For the title_model and text_model, the training data is extracted from the train_dataset using a mapping function to select only the relevant inputs (title or text) and their corresponding labels. Similarly, the validation data is prepared from the val_dataset. These models are trained independently for 10 epochs, with the training progress displayed (verbose=1).\nFor the combined_model, which takes both the title and text inputs, the entire train_dataset is directly used, as it’s already structured to include both inputs. Similarly, the validation data from val_dataset is utilized. This model is also trained for 10 epochs, with training progress displayed.\n# Compile the models\ntitle_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\ntext_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\ncombined_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the models\ntitle_history = title_model.fit(train_dataset.map(lambda x, y: (x[0], y)), validation_data=val_dataset.map(lambda x, y: (x[0], y)), epochs=10, verbose=1)\ntext_history = text_model.fit(train_dataset.map(lambda x, y: (x[1], y)), validation_data=val_dataset.map(lambda x, y: (x[1], y)), epochs=10, verbose=1)\ncombined_history = combined_model.fit(train_dataset, validation_data=val_dataset, epochs=10, verbose=1)\n\n\n\nfrom IPython.display import Image\nImage(filename='/Users/athena/Desktop/ModelTrainingHistory.jpg')\n\n\n\n\n\n\n\n\n\n\n\n\nI used the following code to evaluate all three models. Knowing that model 3 is the best performing, with a validation accuracy of 0.62.\n# Evaluate models on the validation set\ntitle_eval = title_model.evaluate(val_dataset.map(lambda x, y: (x[0], y)))\ntext_eval = text_model.evaluate(val_dataset.map(lambda x, y: (x[1], y)))\ncombined_eval = combined_model.evaluate(val_dataset)\n\n# Print evaluation results\nprint(\"Title Model Evaluation:\")\nprint(\"Loss:\", title_eval[0])\nprint(\"Accuracy:\", title_eval[1])\n\nprint(\"\\nText Model Evaluation:\")\nprint(\"Loss:\", text_eval[0])\nprint(\"Accuracy:\", text_eval[1])\n\nprint(\"\\nCombined Model Evaluation:\")\nprint(\"Loss:\", combined_eval[0])\nprint(\"Accuracy:\", combined_eval[1])\nOutputs:\n45/45 [==============================] - 0s 4ms/step - loss: 4.0216 - accuracy: 0.6726\n45/45 [==============================] - 3s 76ms/step - loss: 2.9807 - accuracy: 0.8914\n45/45 [==============================] - 3s 73ms/step - loss: 2.8842 - accuracy: 0.9869\nTitle Model Evaluation:\nLoss: 4.021571159362793\nAccuracy: 0.6726235061073303\n\nText Model Evaluation:\nLoss: 2.9806952476501465\nAccuracy: 0.8914385829734802\n\nCombined Model Evaluation:\nLoss: 2.8841543197631836\nAccuracy: 0.9869264912605286\n\n\n\n\n\n\nfrom IPython.display import Image\nImage(filename='/Users/athena/Desktop/title_modelSummary().jpg')\n\n\n\n\n\n\n\n\n\n\n\n\nfrom IPython.display import Image\nImage(filename='/Users/athena/Desktop/text_modelSummary().jpg')\n\n\n\n\n\n\n\n\n\n\n\n\nfrom IPython.display import Image\nImage(filename='/Users/athena/Desktop/combined_modelSummary().jpg')"
  },
  {
    "objectID": "posts/fake-news/hw6.html#preprocessing",
    "href": "posts/fake-news/hw6.html#preprocessing",
    "title": "Fake News Classification with Keras",
    "section": "",
    "text": "First, we begin by importing necessary libraries such as Pandas for data manipulation, TensorFlow for building the neural network model, NLTK for natural language processing (stopwords), and scikit-learn for splitting the dataset. Next, we load the dataset from a CSV file hosted on GitHub and download the NLTK stopwords - this will be used later.\nThe function calculate_vocabulary_size is used to compute the size of the vocabulary in the text data, which is the number of unique words. This function is applied to both the title and text columns of the dataset.\nThe function make_dataset preprocesses the text data by converting it to lowercase, removing stopwords, tokenizing, and padding sequences to a fixed length. Following the function, we create a TensorFlow Dataset object from the preprocessed data and splits it into training and validation sets using the train_test_split function from scikit-learn.\nFinally, we calculate the base rate for the dataset, which is the proportion of the majority class (fake or real news) in the training set.\nThe following code block prints the vocabulary size and base rate.\nimport pandas as pd\nimport tensorflow as tf\nfrom keras.layers import Input, Embedding, Dense, Concatenate, Dropout, SimpleRNN, GRU\nfrom keras.models import Model\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Load the dataset\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\ndf = pd.read_csv(train_url)\n\n# Download NLTK stopwords\nimport nltk\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\n# Calculate the size of the vocabulary\ndef calculate_vocabulary_size(text_column):\n    all_words = ' '.join(text_column).split()\n    unique_words = set(all_words)\n    return len(unique_words)\n\n# Calculate the size of the vocabulary for both title and text columns\nsize_vocabulary_title = calculate_vocabulary_size(df['title'])\nsize_vocabulary_text = calculate_vocabulary_size(df['text'])\n\n# Use the maximum vocabulary size from title and text columns\nsize_vocabulary = max(size_vocabulary_title, size_vocabulary_text)\n\nprint(\"Vocabulary Size:\", size_vocabulary)\n\ndef make_dataset(df):\n    # Lowercase the text and title columns\n    df['text'] = df['text'].str.lower()\n    df['title'] = df['title'].str.lower()\n\n    # Remove stopwords from the text and title columns using NLTK\n    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n    df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n\n    # Tokenize the text and title columns\n    tokenizer = Tokenizer(num_words=size_vocabulary)\n    tokenizer.fit_on_texts(df['text'].values)\n    tokenizer.fit_on_texts(df['title'].values)\n\n    # Convert text to sequences and pad them\n    text_sequences = pad_sequences(tokenizer.texts_to_sequences(df['text'].values))\n    title_sequences = pad_sequences(tokenizer.texts_to_sequences(df['title'].values))\n\n    # Combine title and text sequences into a tuple\n    sequences = (title_sequences, text_sequences)\n\n    # Convert the DataFrame to a TensorFlow Dataset\n    dataset = tf.data.Dataset.from_tensor_slices((sequences, df['fake'].values))\n\n    return dataset\n\n# Create datasets\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\ntrain_dataset = make_dataset(train_df).batch(100)\nval_dataset = make_dataset(val_df).batch(100)\n\n# Calculate the base rate for the dataset\nbase_rate = max(train_df['fake'].value_counts(normalize=True))\n\nprint(\"Base Rate:\", base_rate)\nHere is the output:\nVocabulary Size: 276283\nBase Rate: 0.5222451138704828"
  },
  {
    "objectID": "posts/fake-news/hw6.html#model-construction",
    "href": "posts/fake-news/hw6.html#model-construction",
    "title": "Fake News Classification with Keras",
    "section": "",
    "text": "Now we define three separate models for classifying fake news based on the article title, text, or both.\nFirstly, we define the shared layers - an Embedding layer and an RNN layer.\nThen, three models are defined and, for each model, the code follows a similar structure:\n\nfirst define shared layers, which consist of an embedding layer and a recurrent neural network (RNN) layer. The embedding layer converts each word index into a dense vector representation, and the RNN layer processes sequences of these word embeddings.\nThen we define separate inputs for the title and the text, followed by their respective embedding and RNN layers.\nFor the model considering both title and text, we concatenate the outputs of the RNN layers for the title and text inputs.\nFinally, we add a dense layer with a sigmoid activation function, which is typical for binary classification tasks, to produce the final output of each model.\nAdditionally, dropout layers with a dropout rate of 0.5 are inserted after the RNN layers for the text inputs, which helps prevent overfitting by randomly dropping out (setting to zero) a fraction of input units during training.\n\n# Define the shared layers\nembedding_layer = Embedding(input_dim=size_vocabulary, output_dim=100)\nrnn_layer = SimpleRNN(128, return_sequences=True)  # Change LSTM to SimpleRNN or GRU\n\n# Model using only the article title\ntitle_input = Input(shape=(None,))\ntitle_embedding = embedding_layer(title_input)\ntitle_rnn = rnn_layer(title_embedding)\ntitle_rnn = SimpleRNN(128)(title_rnn)  # Change LSTM to SimpleRNN or GRU\ntitle_output = Dense(1, activation='sigmoid')(title_rnn)\n\ntitle_model = Model(inputs=title_input, outputs=title_output)\n\n\n# Model using only the article text\ntext_input = Input(shape=(None,))\ntext_embedding = embedding_layer(text_input)\ntext_rnn = rnn_layer(text_embedding)\ntext_dropout = Dropout(0.5)(text_rnn)  # Add dropout layer with dropout rate of 0.5\ntext_rnn = SimpleRNN(128)(text_dropout)  # Change LSTM to SimpleRNN or GRU\ntext_output = Dense(1, activation='sigmoid')(text_rnn)\n\ntext_model = Model(inputs=text_input, outputs=text_output)\n\n# Model using both article title and text\ntitle_input = Input(shape=(None,))\ntext_input = Input(shape=(None,))\n\ntitle_embedding = embedding_layer(title_input)\ntext_embedding = embedding_layer(text_input)\n\ntitle_rnn = rnn_layer(title_embedding)\ntitle_dropout = Dropout(0.5)(title_rnn)  # Add dropout layer with dropout rate of 0.5\ntitle_rnn = SimpleRNN(128)(title_dropout)  # Change LSTM to SimpleRNN or GRU\n\ntext_rnn = rnn_layer(text_embedding)\ntext_dropout = Dropout(0.5)(text_rnn)  # Add dropout layer with dropout rate of 0.5\ntext_rnn = SimpleRNN(128)(text_dropout)  # Change LSTM to SimpleRNN or GRU\n\nconcatenated = Concatenate()([title_rnn, text_rnn])\ncombined_output = Dense(1, activation='sigmoid')(concatenated)\n\ncombined_model = Model(inputs=[title_input, text_input], outputs=combined_output)"
  },
  {
    "objectID": "posts/fake-news/hw6.html#training",
    "href": "posts/fake-news/hw6.html#training",
    "title": "Fake News Classification with Keras",
    "section": "",
    "text": "Now we compile and train the three models for text classification tasks.\nFirst, the compile() function configures each model for training by specifying the optimizer, loss function, and evaluation metrics. In this case, the Adam optimizer is chosen, along with binary cross-entropy loss, which is typical for binary classification problems, and accuracy as the metric to monitor during training.\nFollowing compilation, the fit() function is called for each model to train them on the provided data. For the title_model and text_model, the training data is extracted from the train_dataset using a mapping function to select only the relevant inputs (title or text) and their corresponding labels. Similarly, the validation data is prepared from the val_dataset. These models are trained independently for 10 epochs, with the training progress displayed (verbose=1).\nFor the combined_model, which takes both the title and text inputs, the entire train_dataset is directly used, as it’s already structured to include both inputs. Similarly, the validation data from val_dataset is utilized. This model is also trained for 10 epochs, with training progress displayed.\n# Compile the models\ntitle_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\ntext_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\ncombined_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the models\ntitle_history = title_model.fit(train_dataset.map(lambda x, y: (x[0], y)), validation_data=val_dataset.map(lambda x, y: (x[0], y)), epochs=10, verbose=1)\ntext_history = text_model.fit(train_dataset.map(lambda x, y: (x[1], y)), validation_data=val_dataset.map(lambda x, y: (x[1], y)), epochs=10, verbose=1)\ncombined_history = combined_model.fit(train_dataset, validation_data=val_dataset, epochs=10, verbose=1)\n\n\n\nfrom IPython.display import Image\nImage(filename='/Users/athena/Desktop/ModelTrainingHistory.jpg')"
  },
  {
    "objectID": "posts/fake-news/hw6.html#evaluation",
    "href": "posts/fake-news/hw6.html#evaluation",
    "title": "Fake News Classification with Keras",
    "section": "",
    "text": "I used the following code to evaluate all three models. Knowing that model 3 is the best performing, with a validation accuracy of 0.62.\n# Evaluate models on the validation set\ntitle_eval = title_model.evaluate(val_dataset.map(lambda x, y: (x[0], y)))\ntext_eval = text_model.evaluate(val_dataset.map(lambda x, y: (x[1], y)))\ncombined_eval = combined_model.evaluate(val_dataset)\n\n# Print evaluation results\nprint(\"Title Model Evaluation:\")\nprint(\"Loss:\", title_eval[0])\nprint(\"Accuracy:\", title_eval[1])\n\nprint(\"\\nText Model Evaluation:\")\nprint(\"Loss:\", text_eval[0])\nprint(\"Accuracy:\", text_eval[1])\n\nprint(\"\\nCombined Model Evaluation:\")\nprint(\"Loss:\", combined_eval[0])\nprint(\"Accuracy:\", combined_eval[1])\nOutputs:\n45/45 [==============================] - 0s 4ms/step - loss: 4.0216 - accuracy: 0.6726\n45/45 [==============================] - 3s 76ms/step - loss: 2.9807 - accuracy: 0.8914\n45/45 [==============================] - 3s 73ms/step - loss: 2.8842 - accuracy: 0.9869\nTitle Model Evaluation:\nLoss: 4.021571159362793\nAccuracy: 0.6726235061073303\n\nText Model Evaluation:\nLoss: 2.9806952476501465\nAccuracy: 0.8914385829734802\n\nCombined Model Evaluation:\nLoss: 2.8841543197631836\nAccuracy: 0.9869264912605286"
  },
  {
    "objectID": "posts/fake-news/hw6.html#model-visualizations",
    "href": "posts/fake-news/hw6.html#model-visualizations",
    "title": "Fake News Classification with Keras",
    "section": "",
    "text": "from IPython.display import Image\nImage(filename='/Users/athena/Desktop/title_modelSummary().jpg')\n\n\n\n\n\n\n\n\n\n\n\n\nfrom IPython.display import Image\nImage(filename='/Users/athena/Desktop/text_modelSummary().jpg')\n\n\n\n\n\n\n\n\n\n\n\n\nfrom IPython.display import Image\nImage(filename='/Users/athena/Desktop/combined_modelSummary().jpg')"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/image-classification/index.html",
    "href": "posts/image-classification/index.html",
    "title": "Image Classification",
    "section": "",
    "text": "The following code imports necessary libraries including os, keras, tensorflow_datasets (abbreviated as tfds), and tensorflow.data (abbreviated as tf_data).\nIt then loads the “cats_vs_dogs” dataset from TensorFlow Datasets (tfds.load()) and splits it into training, validation, and test sets. It allocates 40% of the data for training, 10% for validation, and 10% for testing.\nNext, we resizes all images to a common size of 150x150 pixels using keras.layers.Resizing.\nAfter resizing, it batches and prefetches the datasets (train_ds, validation_ds, test_ds) to improve performance during training. Batching groups multiple images and labels together to be processed simultaneously, and prefetching overlaps data preprocessing and model execution to reduce idle time.\nFinally, it extracts labels from the training dataset (train_ds) using unbatch(), maps them to numpy values, and creates an iterator (labels_iterator) for iterating over the labels.\nimport os\nimport keras\nfrom keras import utils \nimport tensorflow_datasets as tfds\nfrom tensorflow import data as tf_data\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\n\n\nWe define a function visualize_cats_and_dogs() that takes a dataset of images with corresponding labels (presumably from the “cats_vs_dogs” dataset) and visualizes a specified number of random samples of cats and dogs.\n\nFirst, the function collects random samples of cats and dogs from the dataset, where cats have label 0 and dogs have label 1.\nIt then plots these samples in a grid using Matplotlib, with each row representing either cats or dogs. The number of samples per category is specified by the num_samples parameter.\nThe images are plotted with their corresponding titles (“Cat” or “Dog”) and with the axis turned off to remove axis labels.\nThe visualize_cats_and_dogs() function can be called with a dataset as an argument, typically the training dataset (train_ds), and a specified number of samples to visualize.\nThe usage example at the end demonstrates how to call the function with train_ds.take(6), which retrieves the first 6 samples from the training dataset. This visualizes 3 cats and 3 dogs, as specified by the default num_samples parameter.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef visualize_cats_and_dogs(dataset, num_samples=3):\n    cats = []\n    dogs = []\n    \n    # Collect random samples of cats and dogs\n    for image, label in dataset:\n        if label.numpy()[0] == 0:  # Extract integer value from the tensor\n            cats.append(image[0])  # Take the first image from the batch\n        else:  # Dogs have label 1\n            dogs.append(image[0])  # Take the first image from the batch\n        \n        if len(cats) == num_samples and len(dogs) == num_samples:\n            break\n    \n    # Plotting\n    fig, axes = plt.subplots(2, num_samples, figsize=(15, 7))\n    fig.subplots_adjust(hspace=0.3, wspace=0.1)\n    \n    for i in range(min(num_samples, len(cats))):  # Ensure we loop over minimum of requested samples and available samples\n        axes[0, i].imshow(cats[i].numpy().astype(np.uint8))\n        axes[0, i].set_title(\"Cat\")\n        axes[0, i].axis('off')\n    \n    for i in range(min(num_samples, len(dogs))):  # Ensure we loop over minimum of requested samples and available samples\n        axes[1, i].imshow(dogs[i].numpy().astype(np.uint8))\n        axes[1, i].set_title(\"Dog\")\n        axes[1, i].axis('off')\n    \n    plt.show()\n\n# Usage:\nvisualize_cats_and_dogs(train_ds.take(6))\n\n# Initialize counters\ncat_count = 0\ndog_count = 0\n\n# Iterate through the training dataset\nfor image, label in train_ds:\n    if label.numpy()[0] == 0:\n        cat_count += 1\n    else:\n        dog_count += 1\n\nprint(\"Number of cat images:\", cat_count)\nprint(\"Number of dog images:\", dog_count)\n\nfrom IPython.display import Image\nImage(filename='/Users/athena/Desktop/hw5.jpg')\n\n\n\n\n\n\n\n\n\n\n\nTo begin with our first model, we construct a convolutional neural network (CNN) model using TensorFlow’s Keras API to classify images as either cats or dogs.\nThe model comprises a series of convolutional (Conv2D) and max-pooling (MaxPooling2D) layers followed by a flattening layer (Flatten) and two dense layers (Dense) with dropout regularization applied. The input images are expected to have a shape of (150, 150, 3), indicating a width and height of 150 pixels and three color channels (RGB). The activation function used in the convolutional layers is ReLU, and the output layer employs a sigmoid activation function to produce binary classification predictions.\nThe model is compiled with the Adam optimizer and binary cross-entropy loss function. It is then trained on the provided training dataset (train_ds) for 20 epochs while monitoring validation accuracy on the validation dataset (validation_ds). Finally, the training history is visualized by plotting the accuracy and validation accuracy over epochs using Matplotlib. This allows monitoring of the model’s performance and helps in identifying overfitting or underfitting issues during training.\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nmodel1 = tf.keras.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dropout(0.5),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n])\n\nmodel1.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\n# Train the model\nhistory = model1.fit(train_ds, \n                     epochs=20, \n                     validation_data=validation_ds)\n\n# Plot training history\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')\nplt.show()\nIn this model, we experimented with different numbers of filters in the convolutional layers, different dropout rates, and different numbers of units in the dense layer to improve validation accuracy. The validation accuracy of my model during training was around 60-65%. Comparing this to the baseline accuracy of around 50%, my model performs significantly better. Regarding overfitting, we can observe a slight gap between the training and validation accuracies, but it doesn’t seem severe. However, if the gap were to widen significantly in further experiments, it would indicate overfitting. Regularization techniques such as dropout and data augmentation can help mitigate overfitting.\n\n\n\nFor model 2, we construct another convolutional neural network (CNN) model using TensorFlow’s Keras API for image classification tasks. This model (model2) is similar to the previous one but includes data augmentation layers (RandomFlip and RandomRotation) at the beginning. Data augmentation is a technique used to increase the diversity of the training dataset by applying random transformations to the input images, such as flipping horizontally or rotating by a certain factor.\nAfter the data augmentation layers, the model architecture remains the same, comprising convolutional (Conv2D) and max-pooling (MaxPooling2D) layers followed by a flattening layer (Flatten) and two dense layers (Dense) with dropout regularization applied. The input images are expected to have a shape of (150, 150, 3), indicating a width and height of 150 pixels and three color channels (RGB). The activation function used in the convolutional layers is ReLU, and the output layer employs a sigmoid activation function to produce binary classification predictions.\nSimilar to the previous code segment, the model is compiled with the Adam optimizer and binary cross-entropy loss function. It is then trained on the provided training dataset (train_ds) for 20 epochs while monitoring validation accuracy on the validation dataset (validation_ds). Finally, the training history is visualized by plotting the accuracy and validation accuracy over epochs using Matplotlib. This helps monitor the model’s performance and identify any overfitting or underfitting issues during training, taking into account the effects of data augmentation on training dynamics.\nmodel2 = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(mode='horizontal'),\n    tf.keras.layers.RandomRotation(factor=0.1),\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel2.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\n# Train the model\nhistory = model2.fit(train_ds, \n                     epochs=20, \n                     validation_data=validation_ds)\n\n# Plot training history\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')\nplt.show()\nThe validation accuracy of my model during training was around 55-60%. This accuracy is slightly lower than what was achieved with model1. Regarding overfitting, we can see a similar pattern to model1, where there’s a slight gap between the training and validation accuracies. This indicates some level of overfitting, but it’s not severe. Regularization techniques such as dropout can help alleviate overfitting further.\n\n\n\nIn the following code, we first create two visualizations showing the results of applying RandomFlip and RandomRotation to an example image:\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\n# Load a sample image from the dataset\nsample_image, _ = next(iter(train_ds))\n\n# Apply RandomFlip to the sample image\nflip_layer = tf.keras.layers.RandomFlip(\"horizontal\")\nflipped_image = flip_layer(sample_image)\n\n# Apply RandomRotation to the sample image\nrotation_layer = tf.keras.layers.RandomRotation(0.2)\nrotated_image = rotation_layer(sample_image)\n\n# Plot the original, flipped, and rotated images\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 3, 1)\nplt.imshow(sample_image[0].numpy().astype(\"uint8\"))\nplt.title(\"Original\")\nplt.axis(\"off\")\n\nplt.subplot(1, 3, 2)\nplt.imshow(flipped_image[0].numpy().astype(\"uint8\"))\nplt.title(\"Flipped\")\nplt.axis(\"off\")\n\nplt.subplot(1, 3, 3)\nplt.imshow(rotated_image[0].numpy().astype(\"uint8\"))\nplt.title(\"Rotated\")\nplt.axis(\"off\")\n\nplt.show()\n\nfrom IPython.display import Image\nImage(filename='/Users/athena/Desktop/hw5-2.jpg')\n\n\n\n\n\n\n\n\nNow we construct a convolutional neural network (CNN) model using TensorFlow’s Keras API for image classification, incorporating preprocessing and data augmentation techniques - Model 3!\n\nFirst, a preprocessor layer is defined using the keras.Input function to specify the input shape, followed by a keras.layers.Rescaling layer. This layer normalizes pixel values to the range [-1, 1] by dividing by 127.5 and subtracting 1.\nData augmentation layers are defined using keras.Sequential, including keras.layers.RandomFlip for horizontal flipping and keras.layers.RandomRotation for random rotations up to 20 degrees.\nThe rest of the model architecture (model3) is constructed using keras.Sequential, which includes the preprocessor layer, data augmentation layers, convolutional (layers.Conv2D) and max-pooling (layers.MaxPooling2D) layers, a flattening layer (layers.Flatten), dropout regularization (layers.Dropout), and dense layers (layers.Dense) with ReLU activation. The final dense layer uses a sigmoid activation function for binary classification.\nThe model is compiled with the Adam optimizer, binary cross-entropy loss function, and accuracy metric.\nThe model is trained on the provided training dataset (train_ds) for 20 epochs while monitoring validation accuracy on the validation dataset (validation_ds).\nFinally, the training history is visualized by plotting the accuracy and validation accuracy over epochs using Matplotlib, enabling monitoring of the model’s performance and identification of potential overfitting or underfitting issues during training, with consideration of the effects of preprocessing and data augmentation on training dynamics.\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n\n# Define preprocessor layer\ni = keras.Input(shape=(150, 150, 3))\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs=i, outputs=x)\n\n# Define data augmentation layers\ndata_augmentation = keras.Sequential([\n    keras.layers.RandomFlip(\"horizontal\"),\n    keras.layers.RandomRotation(0.2),\n])\n\n# Define the rest of the model\nmodel3 = keras.Sequential([\n    preprocessor,  # Preprocessor layer\n    data_augmentation,  # Data augmentation layers\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dropout(0.5),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel3.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\n# Train the model\nhistory3 = model3.fit(train_ds, \n                      epochs=20, \n                      validation_data=validation_ds)\n\n# Plot training history\nplt.plot(history3.history['accuracy'], label='accuracy')\nplt.plot(history3.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')\nplt.show()\nThe validation accuracy of my model during training ranged between 80% and 85%. The validation accuracy achieved with model3 is significantly higher than the one obtained with model1. There seems to be some overfitting in model3, as the training accuracy is consistently higher than the validation accuracy, and the gap between them widens slightly over epochs. Regularization techniques such as dropout or increasing the size of the validation set could be applied to mitigate this.\n\n\n\nAt last, we construct model 4 - a convolutional neural network (CNN) model using transfer learning with MobileNetV3Large architecture, augmented with additional layers for fine-tuning and classification.\n\nMobileNetV3Large is downloaded and configured as a layer using keras.applications.MobileNetV3Large. It is instantiated with the specified input shape of (150, 150, 3), excluding the top classification layers (include_top=False) and pre-loaded with weights trained on the ImageNet dataset (weights='imagenet'). The base model’s trainable parameters are frozen by setting base_model.trainable = False.\nData augmentation layers are defined using keras.Sequential, including horizontal flipping (keras.layers.RandomFlip) and random rotation up to 20 degrees (keras.layers.RandomRotation).\nThe base_model_layer is created as a Keras model with input i and output x, representing the MobileNetV3Large base model with frozen weights.\nModel model4 is defined using keras.Sequential, consisting of the base_model_layer, data augmentation layers, a global max-pooling layer (keras.layers.GlobalMaxPooling2D), two dense layers (keras.layers.Dense) with ReLU activation and dropout regularization, and a final dense layer with softmax activation for binary classification.\nThe model is compiled with the Adam optimizer, sparse categorical cross-entropy loss function suitable for integer labels, and accuracy metric.\nThe model summary is printed using model4.summary() to provide an overview of the model architecture and parameters.\nThe model is trained on the provided training dataset (train_ds) for 20 epochs while monitoring validation accuracy on the validation dataset (validation_ds).\nFinally, the training history is visualized by plotting the accuracy and validation accuracy over epochs using Matplotlib, allowing monitoring of the model’s performance and identification of potential overfitting or underfitting issues during training.\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Download MobileNetV3Large and configure it as a layer\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                                 include_top=False,\n                                                 weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training=False)\nbase_model_layer = keras.Model(inputs=i, outputs=x)\n\n# Create model4\ndata_augmentation = keras.Sequential([\n    keras.layers.RandomFlip(\"horizontal\"),\n    keras.layers.RandomRotation(0.2),\n])\n\nmodel4 = keras.Sequential([\n    base_model_layer,  # MobileNetV3Large base model\n    data_augmentation,  # Data augmentation layers\n    keras.layers.GlobalMaxPooling2D(),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(2, activation='softmax')  # Dense layer for classification\n])\n\n# Compile the model\nmodel4.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\n\n# Print model summary\nmodel4.summary()\n\n# Train the model\nhistory4 = model4.fit(train_ds, \n                      epochs=20, \n                      validation_data=validation_ds)\n\n# Plot training history\nplt.plot(history4.history['accuracy'], label='accuracy')\nplt.plot(history4.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')\nplt.show()\nThe validation accuracy of my model during training ranged between 93% and 95%. This validation accuracy is significantly higher than the accuracy obtained with model1. Regarding overfitting, model4 seems to perform well, with the validation accuracy closely tracking the training accuracy throughout the epochs. There doesn’t appear to be significant overfitting observed in model4.\n\n\n\ndef evaluate_model(model, dataset):\n    # Evaluate the model on the dataset\n    loss, accuracy = model.evaluate(dataset)\n    return accuracy\n\n# Evaluate all four models on the test dataset\naccuracy_model1 = evaluate_model(model1, test_ds)\naccuracy_model2 = evaluate_model(model2, test_ds)\naccuracy_model3 = evaluate_model(model3, test_ds)\naccuracy_model4 = evaluate_model(model4, test_ds)\n\nprint(\"Test accuracy for model1:\", accuracy_model1)\nprint(\"Test accuracy for model2:\", accuracy_model2)\nprint(\"Test accuracy for model3:\", accuracy_model3)\nprint(\"Test accuracy for model4:\", accuracy_model4)\noutput of above evaluation code:\n37/37 [==============================] - 21s 548ms/step - loss: 0.6934 - accuracy: 0.4940\n37/37 [==============================] - 20s 531ms/step - loss: 0.6712 - accuracy: 0.5602\n37/37 [==============================] - 20s 533ms/step - loss: 0.6351 - accuracy: 0.6328\n37/37 [==============================] - 18s 483ms/step - loss: 0.1257 - accuracy: 0.9721\nTest accuracy for model1: 0.49398109316825867\nTest accuracy for model2: 0.5601891875267029\nTest accuracy for model3: 0.6328461170196533\nTest accuracy for model4: 0.9720550179481506\n\nModel 1: The test accuracy is approximately 49.4%. This model appears to have underperformed compared to the baseline accuracy, which suggests that it might not have learned meaningful patterns in the data.\nModel 2: The test accuracy is around 56.0%. This model performed slightly better than Model 1, but still below the desired threshold of 60%.\nModel 3: The test accuracy is about 63.3%. This model achieved a significant improvement in accuracy compared to Models 1 and 2, but it seems to have slightly underperformed in comparison to Model 4.\nModel 4: The test accuracy is exceptionally high at approximately 97.2%. This model significantly outperformed the other models, indicating that leveraging the pretrained MobileNetV3Large base model led to a highly effective classifier for distinguishing between cats and dogs.\n\nOverall, Model 4 demonstrated the highest test accuracy, showcasing the benefits of leveraging a pretrained model for the task at hand. Models 1, 2, and 3 had varying degrees of performance, with Model 3 achieving the highest accuracy among them.\n\n\n\n\nfrom IPython.display import Image\nImage(filename='/Users/athena/Desktop/hw5_his.jpg')"
  },
  {
    "objectID": "posts/image-classification/index.html#preperations-and-preprocessing",
    "href": "posts/image-classification/index.html#preperations-and-preprocessing",
    "title": "Image Classification",
    "section": "",
    "text": "The following code imports necessary libraries including os, keras, tensorflow_datasets (abbreviated as tfds), and tensorflow.data (abbreviated as tf_data).\nIt then loads the “cats_vs_dogs” dataset from TensorFlow Datasets (tfds.load()) and splits it into training, validation, and test sets. It allocates 40% of the data for training, 10% for validation, and 10% for testing.\nNext, we resizes all images to a common size of 150x150 pixels using keras.layers.Resizing.\nAfter resizing, it batches and prefetches the datasets (train_ds, validation_ds, test_ds) to improve performance during training. Batching groups multiple images and labels together to be processed simultaneously, and prefetching overlaps data preprocessing and model execution to reduce idle time.\nFinally, it extracts labels from the training dataset (train_ds) using unbatch(), maps them to numpy values, and creates an iterator (labels_iterator) for iterating over the labels.\nimport os\nimport keras\nfrom keras import utils \nimport tensorflow_datasets as tfds\nfrom tensorflow import data as tf_data\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()"
  },
  {
    "objectID": "posts/image-classification/index.html#displaying-some-cats-and-dogs",
    "href": "posts/image-classification/index.html#displaying-some-cats-and-dogs",
    "title": "Image Classification",
    "section": "",
    "text": "We define a function visualize_cats_and_dogs() that takes a dataset of images with corresponding labels (presumably from the “cats_vs_dogs” dataset) and visualizes a specified number of random samples of cats and dogs.\n\nFirst, the function collects random samples of cats and dogs from the dataset, where cats have label 0 and dogs have label 1.\nIt then plots these samples in a grid using Matplotlib, with each row representing either cats or dogs. The number of samples per category is specified by the num_samples parameter.\nThe images are plotted with their corresponding titles (“Cat” or “Dog”) and with the axis turned off to remove axis labels.\nThe visualize_cats_and_dogs() function can be called with a dataset as an argument, typically the training dataset (train_ds), and a specified number of samples to visualize.\nThe usage example at the end demonstrates how to call the function with train_ds.take(6), which retrieves the first 6 samples from the training dataset. This visualizes 3 cats and 3 dogs, as specified by the default num_samples parameter.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef visualize_cats_and_dogs(dataset, num_samples=3):\n    cats = []\n    dogs = []\n    \n    # Collect random samples of cats and dogs\n    for image, label in dataset:\n        if label.numpy()[0] == 0:  # Extract integer value from the tensor\n            cats.append(image[0])  # Take the first image from the batch\n        else:  # Dogs have label 1\n            dogs.append(image[0])  # Take the first image from the batch\n        \n        if len(cats) == num_samples and len(dogs) == num_samples:\n            break\n    \n    # Plotting\n    fig, axes = plt.subplots(2, num_samples, figsize=(15, 7))\n    fig.subplots_adjust(hspace=0.3, wspace=0.1)\n    \n    for i in range(min(num_samples, len(cats))):  # Ensure we loop over minimum of requested samples and available samples\n        axes[0, i].imshow(cats[i].numpy().astype(np.uint8))\n        axes[0, i].set_title(\"Cat\")\n        axes[0, i].axis('off')\n    \n    for i in range(min(num_samples, len(dogs))):  # Ensure we loop over minimum of requested samples and available samples\n        axes[1, i].imshow(dogs[i].numpy().astype(np.uint8))\n        axes[1, i].set_title(\"Dog\")\n        axes[1, i].axis('off')\n    \n    plt.show()\n\n# Usage:\nvisualize_cats_and_dogs(train_ds.take(6))\n\n# Initialize counters\ncat_count = 0\ndog_count = 0\n\n# Iterate through the training dataset\nfor image, label in train_ds:\n    if label.numpy()[0] == 0:\n        cat_count += 1\n    else:\n        dog_count += 1\n\nprint(\"Number of cat images:\", cat_count)\nprint(\"Number of dog images:\", dog_count)\n\nfrom IPython.display import Image\nImage(filename='/Users/athena/Desktop/hw5.jpg')"
  },
  {
    "objectID": "posts/image-classification/index.html#model-1",
    "href": "posts/image-classification/index.html#model-1",
    "title": "Image Classification",
    "section": "",
    "text": "To begin with our first model, we construct a convolutional neural network (CNN) model using TensorFlow’s Keras API to classify images as either cats or dogs.\nThe model comprises a series of convolutional (Conv2D) and max-pooling (MaxPooling2D) layers followed by a flattening layer (Flatten) and two dense layers (Dense) with dropout regularization applied. The input images are expected to have a shape of (150, 150, 3), indicating a width and height of 150 pixels and three color channels (RGB). The activation function used in the convolutional layers is ReLU, and the output layer employs a sigmoid activation function to produce binary classification predictions.\nThe model is compiled with the Adam optimizer and binary cross-entropy loss function. It is then trained on the provided training dataset (train_ds) for 20 epochs while monitoring validation accuracy on the validation dataset (validation_ds). Finally, the training history is visualized by plotting the accuracy and validation accuracy over epochs using Matplotlib. This allows monitoring of the model’s performance and helps in identifying overfitting or underfitting issues during training.\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nmodel1 = tf.keras.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dropout(0.5),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n])\n\nmodel1.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\n# Train the model\nhistory = model1.fit(train_ds, \n                     epochs=20, \n                     validation_data=validation_ds)\n\n# Plot training history\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')\nplt.show()\nIn this model, we experimented with different numbers of filters in the convolutional layers, different dropout rates, and different numbers of units in the dense layer to improve validation accuracy. The validation accuracy of my model during training was around 60-65%. Comparing this to the baseline accuracy of around 50%, my model performs significantly better. Regarding overfitting, we can observe a slight gap between the training and validation accuracies, but it doesn’t seem severe. However, if the gap were to widen significantly in further experiments, it would indicate overfitting. Regularization techniques such as dropout and data augmentation can help mitigate overfitting."
  },
  {
    "objectID": "posts/image-classification/index.html#model-2",
    "href": "posts/image-classification/index.html#model-2",
    "title": "Image Classification",
    "section": "",
    "text": "For model 2, we construct another convolutional neural network (CNN) model using TensorFlow’s Keras API for image classification tasks. This model (model2) is similar to the previous one but includes data augmentation layers (RandomFlip and RandomRotation) at the beginning. Data augmentation is a technique used to increase the diversity of the training dataset by applying random transformations to the input images, such as flipping horizontally or rotating by a certain factor.\nAfter the data augmentation layers, the model architecture remains the same, comprising convolutional (Conv2D) and max-pooling (MaxPooling2D) layers followed by a flattening layer (Flatten) and two dense layers (Dense) with dropout regularization applied. The input images are expected to have a shape of (150, 150, 3), indicating a width and height of 150 pixels and three color channels (RGB). The activation function used in the convolutional layers is ReLU, and the output layer employs a sigmoid activation function to produce binary classification predictions.\nSimilar to the previous code segment, the model is compiled with the Adam optimizer and binary cross-entropy loss function. It is then trained on the provided training dataset (train_ds) for 20 epochs while monitoring validation accuracy on the validation dataset (validation_ds). Finally, the training history is visualized by plotting the accuracy and validation accuracy over epochs using Matplotlib. This helps monitor the model’s performance and identify any overfitting or underfitting issues during training, taking into account the effects of data augmentation on training dynamics.\nmodel2 = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(mode='horizontal'),\n    tf.keras.layers.RandomRotation(factor=0.1),\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel2.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\n# Train the model\nhistory = model2.fit(train_ds, \n                     epochs=20, \n                     validation_data=validation_ds)\n\n# Plot training history\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')\nplt.show()\nThe validation accuracy of my model during training was around 55-60%. This accuracy is slightly lower than what was achieved with model1. Regarding overfitting, we can see a similar pattern to model1, where there’s a slight gap between the training and validation accuracies. This indicates some level of overfitting, but it’s not severe. Regularization techniques such as dropout can help alleviate overfitting further."
  },
  {
    "objectID": "posts/image-classification/index.html#model-3",
    "href": "posts/image-classification/index.html#model-3",
    "title": "Image Classification",
    "section": "",
    "text": "In the following code, we first create two visualizations showing the results of applying RandomFlip and RandomRotation to an example image:\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\n# Load a sample image from the dataset\nsample_image, _ = next(iter(train_ds))\n\n# Apply RandomFlip to the sample image\nflip_layer = tf.keras.layers.RandomFlip(\"horizontal\")\nflipped_image = flip_layer(sample_image)\n\n# Apply RandomRotation to the sample image\nrotation_layer = tf.keras.layers.RandomRotation(0.2)\nrotated_image = rotation_layer(sample_image)\n\n# Plot the original, flipped, and rotated images\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 3, 1)\nplt.imshow(sample_image[0].numpy().astype(\"uint8\"))\nplt.title(\"Original\")\nplt.axis(\"off\")\n\nplt.subplot(1, 3, 2)\nplt.imshow(flipped_image[0].numpy().astype(\"uint8\"))\nplt.title(\"Flipped\")\nplt.axis(\"off\")\n\nplt.subplot(1, 3, 3)\nplt.imshow(rotated_image[0].numpy().astype(\"uint8\"))\nplt.title(\"Rotated\")\nplt.axis(\"off\")\n\nplt.show()\n\nfrom IPython.display import Image\nImage(filename='/Users/athena/Desktop/hw5-2.jpg')\n\n\n\n\n\n\n\n\nNow we construct a convolutional neural network (CNN) model using TensorFlow’s Keras API for image classification, incorporating preprocessing and data augmentation techniques - Model 3!\n\nFirst, a preprocessor layer is defined using the keras.Input function to specify the input shape, followed by a keras.layers.Rescaling layer. This layer normalizes pixel values to the range [-1, 1] by dividing by 127.5 and subtracting 1.\nData augmentation layers are defined using keras.Sequential, including keras.layers.RandomFlip for horizontal flipping and keras.layers.RandomRotation for random rotations up to 20 degrees.\nThe rest of the model architecture (model3) is constructed using keras.Sequential, which includes the preprocessor layer, data augmentation layers, convolutional (layers.Conv2D) and max-pooling (layers.MaxPooling2D) layers, a flattening layer (layers.Flatten), dropout regularization (layers.Dropout), and dense layers (layers.Dense) with ReLU activation. The final dense layer uses a sigmoid activation function for binary classification.\nThe model is compiled with the Adam optimizer, binary cross-entropy loss function, and accuracy metric.\nThe model is trained on the provided training dataset (train_ds) for 20 epochs while monitoring validation accuracy on the validation dataset (validation_ds).\nFinally, the training history is visualized by plotting the accuracy and validation accuracy over epochs using Matplotlib, enabling monitoring of the model’s performance and identification of potential overfitting or underfitting issues during training, with consideration of the effects of preprocessing and data augmentation on training dynamics.\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n\n# Define preprocessor layer\ni = keras.Input(shape=(150, 150, 3))\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs=i, outputs=x)\n\n# Define data augmentation layers\ndata_augmentation = keras.Sequential([\n    keras.layers.RandomFlip(\"horizontal\"),\n    keras.layers.RandomRotation(0.2),\n])\n\n# Define the rest of the model\nmodel3 = keras.Sequential([\n    preprocessor,  # Preprocessor layer\n    data_augmentation,  # Data augmentation layers\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dropout(0.5),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel3.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\n# Train the model\nhistory3 = model3.fit(train_ds, \n                      epochs=20, \n                      validation_data=validation_ds)\n\n# Plot training history\nplt.plot(history3.history['accuracy'], label='accuracy')\nplt.plot(history3.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')\nplt.show()\nThe validation accuracy of my model during training ranged between 80% and 85%. The validation accuracy achieved with model3 is significantly higher than the one obtained with model1. There seems to be some overfitting in model3, as the training accuracy is consistently higher than the validation accuracy, and the gap between them widens slightly over epochs. Regularization techniques such as dropout or increasing the size of the validation set could be applied to mitigate this."
  },
  {
    "objectID": "posts/image-classification/index.html#model-4",
    "href": "posts/image-classification/index.html#model-4",
    "title": "Image Classification",
    "section": "",
    "text": "At last, we construct model 4 - a convolutional neural network (CNN) model using transfer learning with MobileNetV3Large architecture, augmented with additional layers for fine-tuning and classification.\n\nMobileNetV3Large is downloaded and configured as a layer using keras.applications.MobileNetV3Large. It is instantiated with the specified input shape of (150, 150, 3), excluding the top classification layers (include_top=False) and pre-loaded with weights trained on the ImageNet dataset (weights='imagenet'). The base model’s trainable parameters are frozen by setting base_model.trainable = False.\nData augmentation layers are defined using keras.Sequential, including horizontal flipping (keras.layers.RandomFlip) and random rotation up to 20 degrees (keras.layers.RandomRotation).\nThe base_model_layer is created as a Keras model with input i and output x, representing the MobileNetV3Large base model with frozen weights.\nModel model4 is defined using keras.Sequential, consisting of the base_model_layer, data augmentation layers, a global max-pooling layer (keras.layers.GlobalMaxPooling2D), two dense layers (keras.layers.Dense) with ReLU activation and dropout regularization, and a final dense layer with softmax activation for binary classification.\nThe model is compiled with the Adam optimizer, sparse categorical cross-entropy loss function suitable for integer labels, and accuracy metric.\nThe model summary is printed using model4.summary() to provide an overview of the model architecture and parameters.\nThe model is trained on the provided training dataset (train_ds) for 20 epochs while monitoring validation accuracy on the validation dataset (validation_ds).\nFinally, the training history is visualized by plotting the accuracy and validation accuracy over epochs using Matplotlib, allowing monitoring of the model’s performance and identification of potential overfitting or underfitting issues during training.\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Download MobileNetV3Large and configure it as a layer\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                                 include_top=False,\n                                                 weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training=False)\nbase_model_layer = keras.Model(inputs=i, outputs=x)\n\n# Create model4\ndata_augmentation = keras.Sequential([\n    keras.layers.RandomFlip(\"horizontal\"),\n    keras.layers.RandomRotation(0.2),\n])\n\nmodel4 = keras.Sequential([\n    base_model_layer,  # MobileNetV3Large base model\n    data_augmentation,  # Data augmentation layers\n    keras.layers.GlobalMaxPooling2D(),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(2, activation='softmax')  # Dense layer for classification\n])\n\n# Compile the model\nmodel4.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\n\n# Print model summary\nmodel4.summary()\n\n# Train the model\nhistory4 = model4.fit(train_ds, \n                      epochs=20, \n                      validation_data=validation_ds)\n\n# Plot training history\nplt.plot(history4.history['accuracy'], label='accuracy')\nplt.plot(history4.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')\nplt.show()\nThe validation accuracy of my model during training ranged between 93% and 95%. This validation accuracy is significantly higher than the accuracy obtained with model1. Regarding overfitting, model4 seems to perform well, with the validation accuracy closely tracking the training accuracy throughout the epochs. There doesn’t appear to be significant overfitting observed in model4."
  },
  {
    "objectID": "posts/image-classification/index.html#evaluating-models",
    "href": "posts/image-classification/index.html#evaluating-models",
    "title": "Image Classification",
    "section": "",
    "text": "def evaluate_model(model, dataset):\n    # Evaluate the model on the dataset\n    loss, accuracy = model.evaluate(dataset)\n    return accuracy\n\n# Evaluate all four models on the test dataset\naccuracy_model1 = evaluate_model(model1, test_ds)\naccuracy_model2 = evaluate_model(model2, test_ds)\naccuracy_model3 = evaluate_model(model3, test_ds)\naccuracy_model4 = evaluate_model(model4, test_ds)\n\nprint(\"Test accuracy for model1:\", accuracy_model1)\nprint(\"Test accuracy for model2:\", accuracy_model2)\nprint(\"Test accuracy for model3:\", accuracy_model3)\nprint(\"Test accuracy for model4:\", accuracy_model4)\noutput of above evaluation code:\n37/37 [==============================] - 21s 548ms/step - loss: 0.6934 - accuracy: 0.4940\n37/37 [==============================] - 20s 531ms/step - loss: 0.6712 - accuracy: 0.5602\n37/37 [==============================] - 20s 533ms/step - loss: 0.6351 - accuracy: 0.6328\n37/37 [==============================] - 18s 483ms/step - loss: 0.1257 - accuracy: 0.9721\nTest accuracy for model1: 0.49398109316825867\nTest accuracy for model2: 0.5601891875267029\nTest accuracy for model3: 0.6328461170196533\nTest accuracy for model4: 0.9720550179481506\n\nModel 1: The test accuracy is approximately 49.4%. This model appears to have underperformed compared to the baseline accuracy, which suggests that it might not have learned meaningful patterns in the data.\nModel 2: The test accuracy is around 56.0%. This model performed slightly better than Model 1, but still below the desired threshold of 60%.\nModel 3: The test accuracy is about 63.3%. This model achieved a significant improvement in accuracy compared to Models 1 and 2, but it seems to have slightly underperformed in comparison to Model 4.\nModel 4: The test accuracy is exceptionally high at approximately 97.2%. This model significantly outperformed the other models, indicating that leveraging the pretrained MobileNetV3Large base model led to a highly effective classifier for distinguishing between cats and dogs.\n\nOverall, Model 4 demonstrated the highest test accuracy, showcasing the benefits of leveraging a pretrained model for the task at hand. Models 1, 2, and 3 had varying degrees of performance, with Model 3 achieving the highest accuracy among them."
  },
  {
    "objectID": "posts/image-classification/index.html#model-history",
    "href": "posts/image-classification/index.html#model-history",
    "title": "Image Classification",
    "section": "",
    "text": "from IPython.display import Image\nImage(filename='/Users/athena/Desktop/hw5_his.jpg')"
  },
  {
    "objectID": "posts/Heat-Equations/hw4.html",
    "href": "posts/Heat-Equations/hw4.html",
    "title": "Heat Diffusion with JAX",
    "section": "",
    "text": "In this project, I will be implementing the heat equation (see image below - credits to Professor Seyoon Ko UCLA PIC16B Winter 2024) through four different methods.\n\nfrom IPython.display import Image\nImage(filename='/Users/athena/Desktop/school/Winter2024/PIC16B/hw4.jpg')\n\n\n\n\n\n\n\n\n\n\nFirst, we import the time module for the rest of the code project to keep track of runtime for different methods. Note that all the following methods are placed inside heat_equation.py.\nimport time\nWe begin by importing the numpy and matplotlib modules. Then the first function is get_A(N) consisting of numpy arrays code that was provided by the professor. The returned A represents a 2D difference matrix \\(N^2 x N^2\\). The second function advance_time_matvecmul(A, u, epsilon) takes in A, u, and epsilon to perform matrix-vector multiplication. The first line (N = …) determines the size of the grid in one dimension, while the next (A @ u.flatten()) performs matrix-vector multiplication on flattened array, which then reshaped to 2D array to calculate the returning value u.\n'''\nPART ONE\n'''\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef get_A(N):\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n - 1), np.ones(n - 1), np.ones(n - N), np.ones(n - N)]\n    diagonals[1][(N - 1)::N] = 0\n    diagonals[2][(N - 1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3],\n                                                                                               N) + np.diag(\n        diagonals[4], -N)\n    return A\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2.\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\ndef visualize_heat(u, iteration):\n    plt.subplot(3, 3, iteration // 300 + 1)\n    plt.imshow(u)\n    plt.title(f'Iteration {iteration}')\n    plt.axis('off')\n    \n# Simulation parameters\nN = 101\nepsilon = 0.2\nnum_iterations = 2700\n\n# Initial condition\nu0 = np.zeros((N, N))\nu0[int(N / 2), int(N / 2)] = 1.0\n\n# Get the matrix A\nA = get_A(N)\nstart_time = time.time()\n\n# Run the simulation\nintermediate_solutions = []\nfor iteration in range(1, num_iterations + 1):\n    u0 = advance_time_matvecmul(A, u0, epsilon)\n    if iteration % 300 == 0:\n        intermediate_solutions.append(u0.copy())\n        visualize_heat(u0, iteration)\n        \nend_time = time.time()\nsimulation_time = end_time - start_time\nprint(\"Simulation time:\", simulation_time)\nThe function visualize_heat basically calls the matplotlib package to display the plots according to the set iterations. Next, we initialize teh parameters N, epsilon, num_iterations, u0, and u0[int(N / 2), int(N / 2)].\nNext, we get the A using get_A and initiate the time recording for the simulation. Then we run the simulation by appending to an empty list intermediate_solutions and using a for-loop to iterate (display graph for every 300 simulations).\nFinally, we stop the runtime recording and print the simulation time. Note that this portion of code is repeated for all following methods with minor distinction in function name and parameter calling - it is primarily used to display the plot, run the simulation, and record the runtime.\nRUNTIME: Executed at 2024.03.02 09:54:39 in 38s 69ms\n\nfrom IPython.display import Image\nImage(filename='/Users/athena/Desktop/part1.jpg')\n\n\n\n\n\n\n\n\n\n\n\nIn this method, we define the function get_sparse_A(N) that returns A_sp_matrix and a jit-ed version of advance_time_matvecmul called advance_time_sparse.\nSimilarly, we first import the jax and scipy modules. The first function get_sparse_A(N) does the following (going line by line): - n calculates the total number of elements in the matrix - constructs a list called diagonals, where each element is a diagnoal for A - set the values of certain diagonals elements to zero - offset for determining relative position to main diagonal (0 is main and positive is right, negative is left) - constructs A_sp_matrix using the diags function with diagonals list, offsets list, shape set to \\(n * n\\) and format to csr\nThe second function advance_time_sparse(A_data, A_indices, A_indptr, u, epsilon) does the following (going line by line): - N to find size of the matrix in 1D - flattens the 2D matrix u into a 1D array to prepare for sparse - parse matrix-vector multiplication using jnp.dot - broadcasts u_result to an array (same shape as u_reshaped) - reshape u_result to match the shape of u - calculate final u matrix\n\"\"\"\nPART TWO\n\"\"\"\nfrom jax import jit\nfrom scipy.sparse import diags\n\ndef get_sparse_A(N):\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n - 1), np.ones(n - 1), np.ones(n - N), np.ones(n - N)]\n    diagonals[1][(N - 1)::N] = 0\n    diagonals[2][(N - 1)::N] = 0\n    \n    A_dense = jnp.diag(diagonals[0])+ jnp.diag(diagonals[1],1)+ jnp.diag(diagonals[2], -1) +jnp.diag(diagonals[3],N)+ jnp.diag(diagonals[4],-N)\n    A_sparse = sparse.BCOO.fromdense(A_dense)\n    return A_sparse\n\n@jit\ndef advance_time_sparse(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: 2d matrix N^2 x N^2\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n# Simulation parameters\nN = 101\nepsilon = 0.2\nnum_iterations = 2700\n\n# Initial condition\nu0 = np.zeros((N, N))\nu0[int(N / 2), int(N / 2)] = 1.0\n\n# Get the sparse matrix A\nA = get_sparse_A(N)\n\n# Run the simulation\nintermediate_solutions_sparse = []\nfor iteration in range(1, num_iterations + 1):\n    u0 = advance_time_sparse(A_data, A_indices, A_indptr, u0, epsilon)\n    if iteration % 300 == 0:\n        intermediate_solutions_sparse.append(u0.copy())\n        visualize_heat(u0, iteration)\nplt.show()\n\nend_time = time.time()\nsimulation_time = end_time - start_time\nprint(\"Simulation time:\", simulation_time)\nThe rest of the code is basically the same as in method 1. However, notice how the sparse matrix A is converted to a format compatible with JAX by ensuring that both the indices and the indptr are int32 type.\nRUNTIME: Executed at 2024.03.02 12:02:17 in 3.9s 315ms\n\nfrom IPython.display import Image\nImage(filename='/Users/athena/Desktop/part2.jpg')\n\n\n\n\n\n\n\n\n\n\n\nIn this method, we define the function advance_time_numpy(u, epsilon) that applies numpy features to tackle the same problem.\nSimilarly, we first import the numpy and matplotlib modules. The only changed function advance_time_numpy(u, epsilon) does the following (going line by line): - n to find the size of matrix in 1D - pads the matrix u using np.pad with a one-cell wide border of zeros - initializes a new matrix u_new with the same shape + data type as u, filled with zeros - iterates through each element of u (excluding the padding) - (code block) updates value for each element in u_new based on the finite difference equation\nThe rest of the code remains similar to method 1 and method 2 - displays plot in end and prints time.\n\"\"\"\nPART THREE\n\"\"\"\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef get_A(N):\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"Advances the JAX simulation via NumPy operations \n    Args:\n        u: The grid state at timestep k.\n        epsilon: The stability constant.\n    Returns:\n        u_new: the updated grid state at time step k+1.\n    \"\"\"\n    n = u.shape[0]\n    u_padded = np.pad(u, 1, mode='constant')\n    u_new = np.zeros_like(u)\n    for i in range(1, n+1):\n        for j in range(1, n+1):\n            u_new[i-1, j-1] = u_padded[i, j] + epsilon * (\n                u_padded[i+1, j] + u_padded[i-1, j] +\n                u_padded[i, j+1] + u_padded[i, j-1] - 4 * u_padded[i, j]\n            )\n    return u_new\n\ndef visualize_heat(u, iteration):\n    plt.subplot(3, 3, iteration // 300 + 1)\n    plt.imshow(u)\n    plt.title(f'Iteration {iteration}')\n    plt.axis('off')\n    \n# Simulation parameters\nN = 101\nepsilon = 0.2\nnum_iterations = 2700\n\n# Initial condition\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n# Get the matrix A\nA = get_A(N)\nstart_time = time.time()\n\n# Run the simulation with numpy operations\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nintermediate_solutions_numpy = []\nfor iteration in range(1, num_iterations + 1):\n    u0 = advance_time_numpy(u0, epsilon)\n    if iteration % 300 == 0:\n        intermediate_solutions_numpy.append(u0.copy())\n        \n# Visualize the diffusion of heat every 300 iterations using numpy operations\nplt.figure(figsize=(10, 10))\nfor i, solution in enumerate(intermediate_solutions_numpy):\n    visualize_heat(solution, (i+1) * 300)\nplt.show()\n\nend_time = time.time()\nsimulation_time = end_time - start_time\nprint(\"Simulation time:\", simulation_time)\nRUNTIME: Executed at 2024.03.02 15:50:22 in 376ms\n\nfrom IPython.display import Image\nImage(filename='/Users/athena/Desktop/part3.jpg')\n\n\n\n\n\n\n\n\n\n\n\nIn this method, we define the function advance_time_jax(u, epsilon) that uses JAX but not the sparse matrix multiplication method.\nWe begin by importing numpy, jax.numpy, jax, and matplotlib modules. The only changed function advance_time_jax(u, epsilon) does the following (going line by line): - n to find the size of matrix in 1D - pads the matrix u using np.pad with a one-cell wide border of zeros using jnp.pad - initializes a new matrix u_new with the same shape + data type as u, filled with zeros - iterates through each element of u (excluding the padding) - (code block) updates value for each element in u_new based on the finite difference equation - creates a mask array to check the indices where the u_new grid should be updated with update_value - uses jnp.where to update u_new where the mask is True\nThe rest of the code remains similar to method 1, 2, and 3 - displays plot in end and prints time.\n\"\"\"\nPART FOUR\n\"\"\"\nimport numpy as np\nimport jax.numpy as jnp\nfrom jax import jit\nfrom matplotlib import pyplot as plt\n\ndef get_A(N):\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\n@jit\ndef advance_time_jax(u, epsilon):\n    n = u.shape[0]\n    u_padded = jnp.pad(u, 1, mode='constant', constant_values = 0)\n    laplacian_u = (jnp.roll(u_padded, 1, axis=0) + jnp.roll(u_padded, -1, axis=0) +\n                  jnp.roll(u_padded, 1, axis=1) + jnp.roll(u_padded, -1, axis=1) -\n                  4* u_padded)[1:-1, 1:-1]\n    u_new = u + epsilon * laplacian_u\n    return u_new\n\ndef visualize_heat(u, iteration):\n    plt.subplot(3, 3, iteration // 300 + 1)\n    plt.imshow(u)\n    plt.title(f'Iteration {iteration}')\n    plt.axis('off')\n    \n# Simulation parameters\nN = 101\nepsilon = 0.2\nnum_iterations = 2700\n\n# Initial condition\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n# Get the matrix A\nA = get_A(N)\nstart_time = time.time()\n\n# Run the simulation with jax\nintermediate_solutions_jax = []\nfor iteration in range(1, num_iterations + 1):\n    u0 = advance_time_jax(u0, epsilon)\n    if iteration % 300 == 0:\n        intermediate_solutions_jax.append(u0.copy())\n        \n# Visualize the diffusion of heat every 300 iterations using jax\nplt.figure(figsize=(10, 10))\nfor i, solution in enumerate(intermediate_solutions_jax):\n    visualize_heat(solution, (i+1) * 300)\nplt.show()\n\nend_time = time.time()\nsimulation_time = end_time - start_time\nprint(\"Simulation time:\", simulation_time)\nRUNTIME: Executed at 2024.03.02 21:17:58 in 181ms\n\nfrom IPython.display import Image\nImage(filename='/Users/athena/Desktop/part4.jpg')\n\n\n\n\n\n\n\n\n\n\n\nIn conclusion, the last method was the fastest, while the numpy method was the easiest for me to write (perhaps due to personal experiences with numpy arrays and how numpy features are more intuitive)."
  },
  {
    "objectID": "posts/Heat-Equations/hw4.html#method-1-matrix-multiplication",
    "href": "posts/Heat-Equations/hw4.html#method-1-matrix-multiplication",
    "title": "Heat Diffusion with JAX",
    "section": "",
    "text": "First, we import the time module for the rest of the code project to keep track of runtime for different methods. Note that all the following methods are placed inside heat_equation.py.\nimport time\nWe begin by importing the numpy and matplotlib modules. Then the first function is get_A(N) consisting of numpy arrays code that was provided by the professor. The returned A represents a 2D difference matrix \\(N^2 x N^2\\). The second function advance_time_matvecmul(A, u, epsilon) takes in A, u, and epsilon to perform matrix-vector multiplication. The first line (N = …) determines the size of the grid in one dimension, while the next (A @ u.flatten()) performs matrix-vector multiplication on flattened array, which then reshaped to 2D array to calculate the returning value u.\n'''\nPART ONE\n'''\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef get_A(N):\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n - 1), np.ones(n - 1), np.ones(n - N), np.ones(n - N)]\n    diagonals[1][(N - 1)::N] = 0\n    diagonals[2][(N - 1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3],\n                                                                                               N) + np.diag(\n        diagonals[4], -N)\n    return A\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2.\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\ndef visualize_heat(u, iteration):\n    plt.subplot(3, 3, iteration // 300 + 1)\n    plt.imshow(u)\n    plt.title(f'Iteration {iteration}')\n    plt.axis('off')\n    \n# Simulation parameters\nN = 101\nepsilon = 0.2\nnum_iterations = 2700\n\n# Initial condition\nu0 = np.zeros((N, N))\nu0[int(N / 2), int(N / 2)] = 1.0\n\n# Get the matrix A\nA = get_A(N)\nstart_time = time.time()\n\n# Run the simulation\nintermediate_solutions = []\nfor iteration in range(1, num_iterations + 1):\n    u0 = advance_time_matvecmul(A, u0, epsilon)\n    if iteration % 300 == 0:\n        intermediate_solutions.append(u0.copy())\n        visualize_heat(u0, iteration)\n        \nend_time = time.time()\nsimulation_time = end_time - start_time\nprint(\"Simulation time:\", simulation_time)\nThe function visualize_heat basically calls the matplotlib package to display the plots according to the set iterations. Next, we initialize teh parameters N, epsilon, num_iterations, u0, and u0[int(N / 2), int(N / 2)].\nNext, we get the A using get_A and initiate the time recording for the simulation. Then we run the simulation by appending to an empty list intermediate_solutions and using a for-loop to iterate (display graph for every 300 simulations).\nFinally, we stop the runtime recording and print the simulation time. Note that this portion of code is repeated for all following methods with minor distinction in function name and parameter calling - it is primarily used to display the plot, run the simulation, and record the runtime.\nRUNTIME: Executed at 2024.03.02 09:54:39 in 38s 69ms\n\nfrom IPython.display import Image\nImage(filename='/Users/athena/Desktop/part1.jpg')"
  },
  {
    "objectID": "posts/Heat-Equations/hw4.html#method-2-sparse-matrix-in-jax",
    "href": "posts/Heat-Equations/hw4.html#method-2-sparse-matrix-in-jax",
    "title": "Heat Diffusion with JAX",
    "section": "",
    "text": "In this method, we define the function get_sparse_A(N) that returns A_sp_matrix and a jit-ed version of advance_time_matvecmul called advance_time_sparse.\nSimilarly, we first import the jax and scipy modules. The first function get_sparse_A(N) does the following (going line by line): - n calculates the total number of elements in the matrix - constructs a list called diagonals, where each element is a diagnoal for A - set the values of certain diagonals elements to zero - offset for determining relative position to main diagonal (0 is main and positive is right, negative is left) - constructs A_sp_matrix using the diags function with diagonals list, offsets list, shape set to \\(n * n\\) and format to csr\nThe second function advance_time_sparse(A_data, A_indices, A_indptr, u, epsilon) does the following (going line by line): - N to find size of the matrix in 1D - flattens the 2D matrix u into a 1D array to prepare for sparse - parse matrix-vector multiplication using jnp.dot - broadcasts u_result to an array (same shape as u_reshaped) - reshape u_result to match the shape of u - calculate final u matrix\n\"\"\"\nPART TWO\n\"\"\"\nfrom jax import jit\nfrom scipy.sparse import diags\n\ndef get_sparse_A(N):\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n - 1), np.ones(n - 1), np.ones(n - N), np.ones(n - N)]\n    diagonals[1][(N - 1)::N] = 0\n    diagonals[2][(N - 1)::N] = 0\n    \n    A_dense = jnp.diag(diagonals[0])+ jnp.diag(diagonals[1],1)+ jnp.diag(diagonals[2], -1) +jnp.diag(diagonals[3],N)+ jnp.diag(diagonals[4],-N)\n    A_sparse = sparse.BCOO.fromdense(A_dense)\n    return A_sparse\n\n@jit\ndef advance_time_sparse(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: 2d matrix N^2 x N^2\n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n# Simulation parameters\nN = 101\nepsilon = 0.2\nnum_iterations = 2700\n\n# Initial condition\nu0 = np.zeros((N, N))\nu0[int(N / 2), int(N / 2)] = 1.0\n\n# Get the sparse matrix A\nA = get_sparse_A(N)\n\n# Run the simulation\nintermediate_solutions_sparse = []\nfor iteration in range(1, num_iterations + 1):\n    u0 = advance_time_sparse(A_data, A_indices, A_indptr, u0, epsilon)\n    if iteration % 300 == 0:\n        intermediate_solutions_sparse.append(u0.copy())\n        visualize_heat(u0, iteration)\nplt.show()\n\nend_time = time.time()\nsimulation_time = end_time - start_time\nprint(\"Simulation time:\", simulation_time)\nThe rest of the code is basically the same as in method 1. However, notice how the sparse matrix A is converted to a format compatible with JAX by ensuring that both the indices and the indptr are int32 type.\nRUNTIME: Executed at 2024.03.02 12:02:17 in 3.9s 315ms\n\nfrom IPython.display import Image\nImage(filename='/Users/athena/Desktop/part2.jpg')"
  },
  {
    "objectID": "posts/Heat-Equations/hw4.html#method-3-direct-operation-with-numpy",
    "href": "posts/Heat-Equations/hw4.html#method-3-direct-operation-with-numpy",
    "title": "Heat Diffusion with JAX",
    "section": "",
    "text": "In this method, we define the function advance_time_numpy(u, epsilon) that applies numpy features to tackle the same problem.\nSimilarly, we first import the numpy and matplotlib modules. The only changed function advance_time_numpy(u, epsilon) does the following (going line by line): - n to find the size of matrix in 1D - pads the matrix u using np.pad with a one-cell wide border of zeros - initializes a new matrix u_new with the same shape + data type as u, filled with zeros - iterates through each element of u (excluding the padding) - (code block) updates value for each element in u_new based on the finite difference equation\nThe rest of the code remains similar to method 1 and method 2 - displays plot in end and prints time.\n\"\"\"\nPART THREE\n\"\"\"\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef get_A(N):\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"Advances the JAX simulation via NumPy operations \n    Args:\n        u: The grid state at timestep k.\n        epsilon: The stability constant.\n    Returns:\n        u_new: the updated grid state at time step k+1.\n    \"\"\"\n    n = u.shape[0]\n    u_padded = np.pad(u, 1, mode='constant')\n    u_new = np.zeros_like(u)\n    for i in range(1, n+1):\n        for j in range(1, n+1):\n            u_new[i-1, j-1] = u_padded[i, j] + epsilon * (\n                u_padded[i+1, j] + u_padded[i-1, j] +\n                u_padded[i, j+1] + u_padded[i, j-1] - 4 * u_padded[i, j]\n            )\n    return u_new\n\ndef visualize_heat(u, iteration):\n    plt.subplot(3, 3, iteration // 300 + 1)\n    plt.imshow(u)\n    plt.title(f'Iteration {iteration}')\n    plt.axis('off')\n    \n# Simulation parameters\nN = 101\nepsilon = 0.2\nnum_iterations = 2700\n\n# Initial condition\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n# Get the matrix A\nA = get_A(N)\nstart_time = time.time()\n\n# Run the simulation with numpy operations\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nintermediate_solutions_numpy = []\nfor iteration in range(1, num_iterations + 1):\n    u0 = advance_time_numpy(u0, epsilon)\n    if iteration % 300 == 0:\n        intermediate_solutions_numpy.append(u0.copy())\n        \n# Visualize the diffusion of heat every 300 iterations using numpy operations\nplt.figure(figsize=(10, 10))\nfor i, solution in enumerate(intermediate_solutions_numpy):\n    visualize_heat(solution, (i+1) * 300)\nplt.show()\n\nend_time = time.time()\nsimulation_time = end_time - start_time\nprint(\"Simulation time:\", simulation_time)\nRUNTIME: Executed at 2024.03.02 15:50:22 in 376ms\n\nfrom IPython.display import Image\nImage(filename='/Users/athena/Desktop/part3.jpg')"
  },
  {
    "objectID": "posts/Heat-Equations/hw4.html#method-4-jax-without-using-sparse-matrix-multiplication",
    "href": "posts/Heat-Equations/hw4.html#method-4-jax-without-using-sparse-matrix-multiplication",
    "title": "Heat Diffusion with JAX",
    "section": "",
    "text": "In this method, we define the function advance_time_jax(u, epsilon) that uses JAX but not the sparse matrix multiplication method.\nWe begin by importing numpy, jax.numpy, jax, and matplotlib modules. The only changed function advance_time_jax(u, epsilon) does the following (going line by line): - n to find the size of matrix in 1D - pads the matrix u using np.pad with a one-cell wide border of zeros using jnp.pad - initializes a new matrix u_new with the same shape + data type as u, filled with zeros - iterates through each element of u (excluding the padding) - (code block) updates value for each element in u_new based on the finite difference equation - creates a mask array to check the indices where the u_new grid should be updated with update_value - uses jnp.where to update u_new where the mask is True\nThe rest of the code remains similar to method 1, 2, and 3 - displays plot in end and prints time.\n\"\"\"\nPART FOUR\n\"\"\"\nimport numpy as np\nimport jax.numpy as jnp\nfrom jax import jit\nfrom matplotlib import pyplot as plt\n\ndef get_A(N):\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\n@jit\ndef advance_time_jax(u, epsilon):\n    n = u.shape[0]\n    u_padded = jnp.pad(u, 1, mode='constant', constant_values = 0)\n    laplacian_u = (jnp.roll(u_padded, 1, axis=0) + jnp.roll(u_padded, -1, axis=0) +\n                  jnp.roll(u_padded, 1, axis=1) + jnp.roll(u_padded, -1, axis=1) -\n                  4* u_padded)[1:-1, 1:-1]\n    u_new = u + epsilon * laplacian_u\n    return u_new\n\ndef visualize_heat(u, iteration):\n    plt.subplot(3, 3, iteration // 300 + 1)\n    plt.imshow(u)\n    plt.title(f'Iteration {iteration}')\n    plt.axis('off')\n    \n# Simulation parameters\nN = 101\nepsilon = 0.2\nnum_iterations = 2700\n\n# Initial condition\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n# Get the matrix A\nA = get_A(N)\nstart_time = time.time()\n\n# Run the simulation with jax\nintermediate_solutions_jax = []\nfor iteration in range(1, num_iterations + 1):\n    u0 = advance_time_jax(u0, epsilon)\n    if iteration % 300 == 0:\n        intermediate_solutions_jax.append(u0.copy())\n        \n# Visualize the diffusion of heat every 300 iterations using jax\nplt.figure(figsize=(10, 10))\nfor i, solution in enumerate(intermediate_solutions_jax):\n    visualize_heat(solution, (i+1) * 300)\nplt.show()\n\nend_time = time.time()\nsimulation_time = end_time - start_time\nprint(\"Simulation time:\", simulation_time)\nRUNTIME: Executed at 2024.03.02 21:17:58 in 181ms\n\nfrom IPython.display import Image\nImage(filename='/Users/athena/Desktop/part4.jpg')"
  },
  {
    "objectID": "posts/Heat-Equations/hw4.html#comparison",
    "href": "posts/Heat-Equations/hw4.html#comparison",
    "title": "Heat Diffusion with JAX",
    "section": "",
    "text": "In conclusion, the last method was the fastest, while the numpy method was the easiest for me to write (perhaps due to personal experiences with numpy arrays and how numpy features are more intuitive)."
  },
  {
    "objectID": "posts/Web-Scraping/HW2.html",
    "href": "posts/Web-Scraping/HW2.html",
    "title": "Web Scraping on Movie",
    "section": "",
    "text": "In this post, I will be presenting my Scrapy project that is used to scrape my favoriate movie: A Rainy Day in New York - directed by Woody Allen.\nPlease click the following link to see my project repository:\n// Note: The following code is used to display the hyperlink\n\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"\"\"&lt;a href=\"https://github.com/Fishier1224/PIC16B_0/tree/main/TMDB_scraper\"&gt;Project Repository&lt;/a&gt;\"\"\"))\n\n/var/folders/4g/1nylr1s57nvcbk28gd1y9r0h0000gn/T/ipykernel_22301/3278939308.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n  from IPython.core.display import display, HTML\n\n\nProject Repository\n\n\n\n\nBegin by entering the following command in your local terminal “scrapy startproject TMDB_scraper”. My project is named TMDB_scraper, but you can name it whatever you wish. This command will create a folder for your scrapy project.\nInside the project folder, you will see another folder named as TMDB_scraper (along with scrapy.cfg). Click in the folder and open settings.py.\nIn settings.py, you want to include the following code to avoid being blocked by the site because they notice you are scraping (403 error). You may need to first install scrapy-fake-useragent using the following command “pip install scrapy-fake-useragent”.\nThe following code in settings.py turns off the built in UserAgentMiddleware and RetryMiddleware, then enables scrapy-fake-useragent’s RandomUserAgentMiddleware and RetryUserAgentMiddleware.\n\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n    'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,\n    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,\n    'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware': 401,\n}\n\n\nFAKEUSERAGENT_PROVIDERS = [\n    'scrapy_fake_useragent.providers.FakeUserAgentProvider',  # Trying first provider\n    'scrapy_fake_useragent.providers.FakerProvider',  # If FakeUserAgentProvider fails, use faker to generate a user-agent string\n    'scrapy_fake_useragent.providers.FixedUserAgentProvider',  # Fall back to USER_AGENT value\n]\n\n## Set Fallback User-Agent\nUSER_AGENT = 'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'\n\nNotice how there is a fallback User-Agent named USER_AGENT, this is the user agent that the program will fall back on if all providers fails.\n\n\n\nFirst, import scrapy. Then create the class TmdbSpider calling “scrapy.Spider”, and assign the name to “tmdb_spider”.\nNext, under init, specify the start_url with “f”https://www.themoviedb.org/movie/{subdir}/““. Notice how the subdirectory is provided with f string. This means that we will need to use “scrapy crawl tmdb_spider -o movies.csv -a subdir=475303-a-rainy-day-in-new-york” when we are calling scrapy in the terminal later on.\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nNow, our first parse function begins with the cast page of the movie that we are scraping. In this function, we yield the scrapy request to call another function parse_full_credits, which will be used to scape individual casts in the movie. Notice how the url is simply the self.subdir + /cast.\n\n   def parse(self, response):\n        \"\"\"\n        The main parse function.\n        sets the url to the cast page of the movie, then calls parse_full_credits to\n        proceed to individual actors (casts) of the movie.\n        return: N/A\n        \"\"\"\n        yield scrapy.Request(f\"https://www.themoviedb.org/movie/{self.subdir}/cast\",\n                             callback=self.parse_full_credits)\n\nFollowing, we have the parse_full_credits function. Since the cast actor links on the website html are placed within the li element (written like this in the html: ol.people.credits:not(.crew) li), we first select that portion and assign it to actors (note how we also did not(.crew) to avoid also getting the crew cast.\nThen we iterate through the actors to get individual links to the actor’s page using actor.css(‘a::attr(href)’).get(). Next, we check if we are on the actor’s page with if actor_link and calls to the function parse_actor_page for continued parsing.\n\n    def parse_full_credits(self, response):\n        \"\"\"\n        Parsing for actors in the movie, provides link for each.\n        iterates through the &lt;li&gt; elements to get href attributes (link to each actor)\n        calls parse_actor_page() when the actor page is reached.\n        return: N/A\n        \"\"\"\n        actors = response.css('ol.people.credits:not(.crew) li')   # Select the &lt;li&gt; elements containing actor information\n        for actor in actors:\n            actor_link = actor.css('a::attr(href)').get()  # Extract the href attribute of the &lt;a&gt; tag\n            if actor_link:\n                yield scrapy.Request(response.urljoin(actor_link), callback=self.parse_actor_page)\n\nThis is our final parsing function - parse_actor_page. This function assumes that we are at an actor’s page, and begin by extracting the actor’s name. Notice how there is a split in the first line. This is because the website’s html includes an index number for actor’s name and we don’t want that.\nThe second line selects the acting section of the page, where all the actor’s acting works are located. The third line then specifies an xpath, while the four further specifies the xpath in line three to the titles containing “role”. Finally, we iterate through the titles to yield a dictionary where the key is the actor’s name, and the value is the title of individual works that they are known for.\n\n    def parse_actor_page(self, response):\n        \"\"\"\n        Assumes at an actor page. Extracts the actor's name using title::text.\n        For a single actor, extracts the title of their works under the \"known_for\" section\n        iterates through the title to yield a dictionary with actor's name and the name of their work\n        \"\"\"\n        actor_name = response.css('title::text').get().split(' — ')[0] # Split to get rid of following number index\n        acting_section = response.css('h3:contains(\"Acting\")')\n        table = acting_section.xpath('./following-sibling::table').get()\n        titles = scrapy.Selector(text=table).xpath('.//td[contains(@class, \"role\")]/a/bdi/text()').getall()\n        for title in titles:\n            yield {\"actor\": actor_name, 'movie_or_TV_name': title}\n\n\n\n\nNow that we finished implementing the class and three parse functions. Type in the terminal under the same directory as “TMDB_scraper”: scrapy crawl tmdb_spider -o results.csv -a subdir=475303-a-rainy-day-in-new-york.\nThis line will scrape your website and generate a result.csv file that presents the dictionary.\n\n\n\nNow that we have the results.csv, read the csv file using pandas:\n\nimport pandas as pd\nresults = pd.read_csv(\"results.csv\")\n\nHere is a overview of what the result looks like using “results.head()”:\n\nresults.head()\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nTimothée Chalamet\nCall Me by Your Name\n\n\n1\nTimothée Chalamet\nDune\n\n\n2\nTimothée Chalamet\nThe King\n\n\n3\nTimothée Chalamet\nInterstellar\n\n\n4\nTimothée Chalamet\nA Rainy Day in New York\n\n\n\n\n\n\n\nNow we are going to use matplotlib for an interesting visualization. Begin by importing matplotlib:\n\nimport matplotlib.pyplot as plt\n\nNow we create a new dataframe based on the results dataframe, named frequency_df. This line groups the results dataframe by ‘movie_or_TV_name’ and count the number of unique ‘actor’ values (the frequency of actors that shares the same ‘movie_or_TV_name’).\n\nfrequency_df = results.groupby('movie_or_TV_name')['actor'].nunique().reset_index()\n\nHere we rename the columns of our new data frame to “actor” and “frequency”. Then the next line sorts the frequency_df by the ‘frequency’ column in descending order (organizing the movies with the highest frequency to the top of the dataframe).\nThe third line rests the index to make the dataframe more presentable.\n\nfrequency_df = frequency_df.rename(columns={'actor': 'frequency'})\nfrequency_df = frequency_df.sort_values('frequency', ascending=False)\nfrequency_df = frequency_df.reset_index(drop=True)\n\nNext, to prepare for better plotting, we specify the frequency_df to only include the top 10 highest frequency movies.\n\nfrequency_df = frequency_df.head(10)\n\nNow we generate the bar chart! First call the x and y variables as “movie_or_TV_name” and “frequency” in the frequency_df dataframe. Then, label the x and y axis as “Movie or TV Name” and “Frequency”. Finally, we label the plot’s title and set the x-axis ticks as “rotation=90”.\nAt last we display the chart!\n\nplt.bar(frequency_df['movie_or_TV_name'], frequency_df['frequency'])\nplt.xlabel('Movie or TV Name')\nplt.ylabel('Frequency')\nplt.title('Frequency of Actors Sharing the Same Movie or TV Name')\nplt.xticks(rotation=90)\nplt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "posts/Web-Scraping/HW2.html#preparing-for-scraping",
    "href": "posts/Web-Scraping/HW2.html#preparing-for-scraping",
    "title": "Web Scraping on Movie",
    "section": "",
    "text": "Begin by entering the following command in your local terminal “scrapy startproject TMDB_scraper”. My project is named TMDB_scraper, but you can name it whatever you wish. This command will create a folder for your scrapy project.\nInside the project folder, you will see another folder named as TMDB_scraper (along with scrapy.cfg). Click in the folder and open settings.py.\nIn settings.py, you want to include the following code to avoid being blocked by the site because they notice you are scraping (403 error). You may need to first install scrapy-fake-useragent using the following command “pip install scrapy-fake-useragent”.\nThe following code in settings.py turns off the built in UserAgentMiddleware and RetryMiddleware, then enables scrapy-fake-useragent’s RandomUserAgentMiddleware and RetryUserAgentMiddleware.\n\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n    'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,\n    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,\n    'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware': 401,\n}\n\n\nFAKEUSERAGENT_PROVIDERS = [\n    'scrapy_fake_useragent.providers.FakeUserAgentProvider',  # Trying first provider\n    'scrapy_fake_useragent.providers.FakerProvider',  # If FakeUserAgentProvider fails, use faker to generate a user-agent string\n    'scrapy_fake_useragent.providers.FixedUserAgentProvider',  # Fall back to USER_AGENT value\n]\n\n## Set Fallback User-Agent\nUSER_AGENT = 'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'\n\nNotice how there is a fallback User-Agent named USER_AGENT, this is the user agent that the program will fall back on if all providers fails."
  },
  {
    "objectID": "posts/Web-Scraping/HW2.html#implementing-the-tmdbspider-class",
    "href": "posts/Web-Scraping/HW2.html#implementing-the-tmdbspider-class",
    "title": "Web Scraping on Movie",
    "section": "",
    "text": "First, import scrapy. Then create the class TmdbSpider calling “scrapy.Spider”, and assign the name to “tmdb_spider”.\nNext, under init, specify the start_url with “f”https://www.themoviedb.org/movie/{subdir}/““. Notice how the subdirectory is provided with f string. This means that we will need to use “scrapy crawl tmdb_spider -o movies.csv -a subdir=475303-a-rainy-day-in-new-york” when we are calling scrapy in the terminal later on.\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nNow, our first parse function begins with the cast page of the movie that we are scraping. In this function, we yield the scrapy request to call another function parse_full_credits, which will be used to scape individual casts in the movie. Notice how the url is simply the self.subdir + /cast.\n\n   def parse(self, response):\n        \"\"\"\n        The main parse function.\n        sets the url to the cast page of the movie, then calls parse_full_credits to\n        proceed to individual actors (casts) of the movie.\n        return: N/A\n        \"\"\"\n        yield scrapy.Request(f\"https://www.themoviedb.org/movie/{self.subdir}/cast\",\n                             callback=self.parse_full_credits)\n\nFollowing, we have the parse_full_credits function. Since the cast actor links on the website html are placed within the li element (written like this in the html: ol.people.credits:not(.crew) li), we first select that portion and assign it to actors (note how we also did not(.crew) to avoid also getting the crew cast.\nThen we iterate through the actors to get individual links to the actor’s page using actor.css(‘a::attr(href)’).get(). Next, we check if we are on the actor’s page with if actor_link and calls to the function parse_actor_page for continued parsing.\n\n    def parse_full_credits(self, response):\n        \"\"\"\n        Parsing for actors in the movie, provides link for each.\n        iterates through the &lt;li&gt; elements to get href attributes (link to each actor)\n        calls parse_actor_page() when the actor page is reached.\n        return: N/A\n        \"\"\"\n        actors = response.css('ol.people.credits:not(.crew) li')   # Select the &lt;li&gt; elements containing actor information\n        for actor in actors:\n            actor_link = actor.css('a::attr(href)').get()  # Extract the href attribute of the &lt;a&gt; tag\n            if actor_link:\n                yield scrapy.Request(response.urljoin(actor_link), callback=self.parse_actor_page)\n\nThis is our final parsing function - parse_actor_page. This function assumes that we are at an actor’s page, and begin by extracting the actor’s name. Notice how there is a split in the first line. This is because the website’s html includes an index number for actor’s name and we don’t want that.\nThe second line selects the acting section of the page, where all the actor’s acting works are located. The third line then specifies an xpath, while the four further specifies the xpath in line three to the titles containing “role”. Finally, we iterate through the titles to yield a dictionary where the key is the actor’s name, and the value is the title of individual works that they are known for.\n\n    def parse_actor_page(self, response):\n        \"\"\"\n        Assumes at an actor page. Extracts the actor's name using title::text.\n        For a single actor, extracts the title of their works under the \"known_for\" section\n        iterates through the title to yield a dictionary with actor's name and the name of their work\n        \"\"\"\n        actor_name = response.css('title::text').get().split(' — ')[0] # Split to get rid of following number index\n        acting_section = response.css('h3:contains(\"Acting\")')\n        table = acting_section.xpath('./following-sibling::table').get()\n        titles = scrapy.Selector(text=table).xpath('.//td[contains(@class, \"role\")]/a/bdi/text()').getall()\n        for title in titles:\n            yield {\"actor\": actor_name, 'movie_or_TV_name': title}"
  },
  {
    "objectID": "posts/Web-Scraping/HW2.html#running-the-spider",
    "href": "posts/Web-Scraping/HW2.html#running-the-spider",
    "title": "Web Scraping on Movie",
    "section": "",
    "text": "Now that we finished implementing the class and three parse functions. Type in the terminal under the same directory as “TMDB_scraper”: scrapy crawl tmdb_spider -o results.csv -a subdir=475303-a-rainy-day-in-new-york.\nThis line will scrape your website and generate a result.csv file that presents the dictionary."
  },
  {
    "objectID": "posts/Web-Scraping/HW2.html#some-visualization",
    "href": "posts/Web-Scraping/HW2.html#some-visualization",
    "title": "Web Scraping on Movie",
    "section": "",
    "text": "Now that we have the results.csv, read the csv file using pandas:\n\nimport pandas as pd\nresults = pd.read_csv(\"results.csv\")\n\nHere is a overview of what the result looks like using “results.head()”:\n\nresults.head()\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nTimothée Chalamet\nCall Me by Your Name\n\n\n1\nTimothée Chalamet\nDune\n\n\n2\nTimothée Chalamet\nThe King\n\n\n3\nTimothée Chalamet\nInterstellar\n\n\n4\nTimothée Chalamet\nA Rainy Day in New York\n\n\n\n\n\n\n\nNow we are going to use matplotlib for an interesting visualization. Begin by importing matplotlib:\n\nimport matplotlib.pyplot as plt\n\nNow we create a new dataframe based on the results dataframe, named frequency_df. This line groups the results dataframe by ‘movie_or_TV_name’ and count the number of unique ‘actor’ values (the frequency of actors that shares the same ‘movie_or_TV_name’).\n\nfrequency_df = results.groupby('movie_or_TV_name')['actor'].nunique().reset_index()\n\nHere we rename the columns of our new data frame to “actor” and “frequency”. Then the next line sorts the frequency_df by the ‘frequency’ column in descending order (organizing the movies with the highest frequency to the top of the dataframe).\nThe third line rests the index to make the dataframe more presentable.\n\nfrequency_df = frequency_df.rename(columns={'actor': 'frequency'})\nfrequency_df = frequency_df.sort_values('frequency', ascending=False)\nfrequency_df = frequency_df.reset_index(drop=True)\n\nNext, to prepare for better plotting, we specify the frequency_df to only include the top 10 highest frequency movies.\n\nfrequency_df = frequency_df.head(10)\n\nNow we generate the bar chart! First call the x and y variables as “movie_or_TV_name” and “frequency” in the frequency_df dataframe. Then, label the x and y axis as “Movie or TV Name” and “Frequency”. Finally, we label the plot’s title and set the x-axis ticks as “rotation=90”.\nAt last we display the chart!\n\nplt.bar(frequency_df['movie_or_TV_name'], frequency_df['frequency'])\nplt.xlabel('Movie or TV Name')\nplt.ylabel('Frequency')\nplt.title('Frequency of Actors Sharing the Same Movie or TV Name')\nplt.xticks(rotation=90)\nplt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Fake News Classification with Keras\n\n\n\n\n\n\nHW6\n\n\nKeras\n\n\nPandas\n\n\nMatplotlib\n\n\nTensorflow\n\n\nnltk\n\n\nscikit-learn\n\n\n\n\n\n\n\n\n\nMar 5, 2024\n\n\nAthena Mo\n\n\n\n\n\n\n\n\n\n\n\n\nImage Classification\n\n\n\n\n\n\nHW5\n\n\nTensorflow\n\n\nKeras\n\n\nNumpy\n\n\n\n\n\n\n\n\n\nMar 1, 2024\n\n\nAthena Mo\n\n\n\n\n\n\n\n\n\n\n\n\nHeat Diffusion with JAX\n\n\n\n\n\n\nHW4\n\n\nJAX\n\n\nNumPy\n\n\nMatplotlib\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nAthena Mo\n\n\n\n\n\n\n\n\n\n\n\n\nFlask Messages\n\n\n\n\n\n\nHW3\n\n\nflask\n\n\nSQL\n\n\n\n\n\n\n\n\n\nFeb 14, 2024\n\n\nAthena Mo\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping on Movie\n\n\n\n\n\n\nHW2\n\n\nscrapy\n\n\npandas\n\n\nmatplotlib\n\n\n\n\n\n\n\n\n\nFeb 1, 2024\n\n\nAthena Mo\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins Data Visualization\n\n\n\n\n\n\nHW0\n\n\npandas\n\n\nplotly\n\n\nmatplotlib\n\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\nAthena Mo\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  }
]